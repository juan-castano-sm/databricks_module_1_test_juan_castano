{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a935c0-a3d7-4367-81a6-4f9ba1b30fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 1: Databricks Lakehouse Platform Fundamentals \n",
    "## Laboratory Exercises\n",
    "\n",
    "Welcome to the hands-on laboratory exercises for Module 1! This notebook contains all the practical exercises referenced in the student materials.\n",
    "\n",
    "### Prerequisites\n",
    "- Access to a Databricks workspace\n",
    "- Basic familiarity with Python\n",
    "- Approximately 2.5 hours for completion\n",
    "\n",
    "### Lab Structure\n",
    "1. **Lab 1**: Exploring Your Databricks Workspace (30 minutes)\n",
    "2. **Lab 2**: Creating and Managing Compute Resources (45 minutes)\n",
    "3. **Lab 3**: Working with DBFS and Delta Lake (45 minutes)\n",
    "4. **Lab 4**: Collaboration and Development Workflows (30 minutes)\n",
    "\n",
    "### Instructions\n",
    "- Complete the labs in order as they build upon each other\n",
    "- Run each cell sequentially within a section\n",
    "- Take time to understand the output before proceeding\n",
    "- Document any issues or questions for the Wednesday follow-up session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b02814a2-2ec7-489c-9a50-19c2832ccbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 1: Exploring Your Databricks Workspace (30 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Navigate the Databricks interface\n",
    "- Create your personal workspace structure\n",
    "- Understand notebooks and their capabilities\n",
    "- Set up your learning environment\n",
    "\n",
    "### Note\n",
    "Some exercises in this lab require UI interaction. Follow the instructions and use the code cells where provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b152eb-07fe-414a-8890-e284460bc7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.1: Workspace Setup\n",
    "\n",
    "🟢 **Required Steps:**\n",
    "1. Log into your Databricks workspace\n",
    "2. Navigate to your user folder under Workspace > Users > [your-email]\n",
    "3. Create a folder structure:\n",
    "   - Right-click your user folder\n",
    "   - Create folder: `Databricks_Course`\n",
    "   - Inside that, create: `Module_01_Fundamentals`\n",
    "4. Upload this notebook to the Module_01_Fundamentals folder\n",
    "\n",
    "Once complete, run the following cell to verify your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09c364b-1057-48c1-a039-e6da1f88c181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify Databricks environment\n",
    "print(\"Hello, Databricks!\")\n",
    "print(f\"Current date: {__import__('datetime').datetime.now()}\")\n",
    "print(f\"Python version: {__import__('sys').version}\")\n",
    "\n",
    "# Note: This cell will only run after attaching to a cluster (Lab 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f2ff7b-7059-4521-b2f2-847a910c111e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.2: Understanding Notebook Features\n",
    "\n",
    "Notebooks support multiple languages and cell types. Let's explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8588cdea-3350-4b6d-a7c4-1d977f376cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python cell - Default language\n",
    "welcome_message = \"Welcome to Databricks!\"\n",
    "print(welcome_message)\n",
    "\n",
    "# Create a simple function\n",
    "def greet_user(name):\n",
    "    return f\"Hello {name}, welcome to the Lakehouse journey!\"\n",
    "\n",
    "print(greet_user(\"Data Engineer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0631cf42-e900-4084-86d0-98c186a6b031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL cell - Using magic command\n",
    "-- This will work after cluster attachment\n",
    "SELECT 'SQL is also available in notebooks!' as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec345f5f-8bf1-44b7-aab9-9d9402e83c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Scala cell - Another language option\n",
    "println(\"Scala is available too!\")\n",
    "val courseModules = List(\"Fundamentals\", \"ETL\", \"Delta Lake\", \"Production\", \"Governance\")\n",
    "courseModules.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade3d1eb-3e38-43f4-a611-09a739e6c319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.3: Exploring Databricks Utilities\n",
    "\n",
    "Databricks provides utilities (dbutils) for file system operations, secrets, and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c782214e-a804-47c0-8bbc-3b9e3dfe167a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore available utilities\n",
    "# Note: Requires cluster attachment\n",
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafa772c-ccbb-4614-b964-df756d1de95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get help on specific utility\n",
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce74df1-6870-4b05-b880-66a0147beb86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 1 Checkpoint\n",
    "\n",
    "By now you should have:\n",
    "- ✅ Successfully logged into Databricks\n",
    "- ✅ Created your course folder structure\n",
    "- ✅ Uploaded and opened this notebook\n",
    "- ✅ Understood basic notebook operations\n",
    "\n",
    "**Note**: Code execution requires a cluster, which we'll create in Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4588fb02-37cb-46c9-8fbf-7f8edbced000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 2: Creating and Managing Compute Resources (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Understand different types of compute resources\n",
    "- Create and configure your first cluster\n",
    "- Learn cost optimization strategies\n",
    "- Practice cluster management operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aeb0e42-644f-45f4-8290-c96c6ece978b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.1: Create Your Learning Cluster\n",
    "\n",
    "🟢 **Required Steps:**\n",
    "\n",
    "1. Navigate to **Compute** in the left sidebar\n",
    "2. Click **Create Cluster**\n",
    "3. Configure with these settings:\n",
    "   - **Cluster Name**: `learning-[yourname]-module01`\n",
    "   - **Cluster Mode**: Standard\n",
    "   - **Databricks Runtime**: Latest LTS version (e.g., 14.3 LTS)\n",
    "   - **Worker Type**: \n",
    "     - AWS: `i3.xlarge`\n",
    "     - Azure: `Standard_DS3_v2`\n",
    "     - GCP: `n1-standard-4`\n",
    "   - **Workers**: Min 1, Max 2\n",
    "   - **Enable autoscaling**: ✓\n",
    "   - **Auto termination**: 120 minutes\n",
    "   - **Enable spot instances**: ✓ (for cost savings)\n",
    "4. Click **Create Cluster**\n",
    "5. Wait for cluster to start (5-7 minutes)\n",
    "\n",
    "While waiting, review the configuration options and their implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881ba922-9de7-49a6-b40d-3849ec55f968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.2: Attach Notebook to Cluster\n",
    "\n",
    "Once your cluster is running:\n",
    "1. At the top of this notebook, click the **Detached** dropdown\n",
    "2. Select your newly created cluster\n",
    "3. Wait for \"Connected\" status\n",
    "4. Run the following verification cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f6c0e7e-79c4-47cf-8089-527a2d33e0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify cluster attachment and Spark session\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python version: {sc.pythonVer}\")\n",
    "print(f\"Cluster: {spark.conf.get('spark.databricks.clusterUsageTags.clusterName')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7aec693-f9ba-47b4-ba92-649e6ac60e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.3: Understanding Cluster Metrics\n",
    "\n",
    "Let's run some basic operations to see cluster metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41e1f6d-b13d-4837-878f-b463c6fa0da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple DataFrame to test cluster\n",
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "# Generate sample data\n",
    "data = [Row(id=i, value=random.random()) for i in range(1000000)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.createOrReplaceTempView(\"test_data\")\n",
    "# Perform operations\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "num_partitions = spark.sql(\"SELECT spark_partition_id() as partition_id FROM test_data\").distinct().count()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "# Simple aggregation\n",
    "result = df.agg({\"value\": \"avg\"}).collect()[0][0]\n",
    "print(f\"Average value: {result:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61b54c1-134e-46a9-944d-84a7875afef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View Spark UI (opens in new tab)\n",
    "displayHTML(\"\"\"\n",
    "<h4>Cluster Monitoring</h4>\n",
    "<p>Click the <b>Spark UI</b> tab at the top of this notebook to see:</p>\n",
    "<ul>\n",
    "  <li>Job execution details</li>\n",
    "  <li>Stage information</li>\n",
    "  <li>Executor status</li>\n",
    "  <li>Environment configurations</li>\n",
    "</ul>\n",
    "<p>This is invaluable for performance tuning!</p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c02bcb4-7564-4985-a29d-30325d4518dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.4: Cluster Management Operations\n",
    "\n",
    "🟡 **Practice these operations from the Compute UI:**\n",
    "\n",
    "1. **View Event Log**: Check cluster startup events\n",
    "2. **Metrics**: Monitor CPU, memory usage\n",
    "3. **Configuration**: Review and understand settings\n",
    "4. **Clone**: Create a copy of cluster config\n",
    "\n",
    "🔴 **Important**: Don't terminate the cluster yet - we need it for remaining labs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ff900e-d586-4582-9517-05b53482ba2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 2 Checkpoint\n",
    "\n",
    "You should now have:\n",
    "- ✅ Successfully created a cluster\n",
    "- ✅ Attached this notebook to the cluster\n",
    "- ✅ Verified Spark is running\n",
    "- ✅ Understood basic cluster operations\n",
    "- ✅ Explored the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c201fb0-60cd-46fa-a3d5-8f17aeda7c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 3: Working with DBFS and Delta Lake (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Explore DBFS and understand its relationship to cloud storage\n",
    "- Read various file formats using Spark\n",
    "- Create your first Delta table\n",
    "- Practice time travel queries\n",
    "- Understand transaction logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "620036f5-eb9b-4616-bd39-a4c52dbd1cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.1: Exploring DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3083734c-9a71-40b9-a485-d4b7cf55c2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List DBFS root contents\n",
    "display(dbutils.fs.ls(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f500c4-eeff-4204-84ee-05b2277cb843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore sample datasets\n",
    "display(dbutils.fs.ls(\"/databricks-datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82ccf51-0315-45e7-bdbc-cb392649e5c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at retail dataset structure\n",
    "display(dbutils.fs.ls(\"/databricks-datasets/retail-org\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25f45e4-2a97-4ca0-a26c-c55bdde866b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check file details\n",
    "dbutils.fs.head(\"/databricks-datasets/retail-org/customers/customers.csv\", max_bytes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbf4d1d-5c65-49b6-8b51-df50076d4cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.2: Reading Different File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c5cc01-9d57-4a18-85a4-e081b0187177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "customers_csv = spark.read.csv(\n",
    "    \"/databricks-datasets/retail-org/customers/customers.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"CSV Records: {customers_csv.count():,}\")\n",
    "customers_csv.printSchema()\n",
    "display(customers_csv.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835e2c6c-9b8b-4c2c-b808-1da3fc3b6dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read JSON file (if available)\n",
    "try:\n",
    "    events_json = spark.read.json(\"/databricks-datasets/structured-streaming/events\")\n",
    "    print(f\"JSON Records: {events_json.count():,}\")\n",
    "    display(events_json.limit(5))\n",
    "except:\n",
    "    print(\"JSON dataset not available, creating sample...\")\n",
    "    # Create sample JSON data\n",
    "    json_data = spark.range(100).selectExpr(\n",
    "        \"id\",\n",
    "        \"rand() * 100 as value\",\n",
    "        \"current_timestamp() as timestamp\"\n",
    "    )\n",
    "    display(json_data.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818b2eec-3cc6-496f-b2f6-16a1a1ff2fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare file format performance\n",
    "import time\n",
    "\n",
    "# Write same data in different formats\n",
    "test_data = spark.range(1000000).selectExpr(\"id\", \"rand() * 1000 as value\")\n",
    "\n",
    "# CSV write/read\n",
    "csv_path = \"/tmp/format_test.csv\"\n",
    "start = time.time()\n",
    "test_data.write.mode(\"overwrite\").csv(csv_path)\n",
    "csv_write_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "spark.read.csv(csv_path).count()\n",
    "csv_read_time = time.time() - start\n",
    "\n",
    "# Parquet write/read\n",
    "parquet_path = \"/tmp/format_test.parquet\"\n",
    "start = time.time()\n",
    "test_data.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "parquet_write_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "spark.read.parquet(parquet_path).count()\n",
    "parquet_read_time = time.time() - start\n",
    "\n",
    "print(\"Format Performance Comparison:\")\n",
    "print(f\"CSV - Write: {csv_write_time:.2f}s, Read: {csv_read_time:.2f}s\")\n",
    "print(f\"Parquet - Write: {parquet_write_time:.2f}s, Read: {parquet_read_time:.2f}s\")\n",
    "print(f\"\\nParquet is {csv_read_time/parquet_read_time:.1f}x faster for reads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536e5ac3-1d2d-4ba7-bb68-122f7e27f00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.3: Creating Your First Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58c642c-ce05-4506-ad2d-d790e9b45d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a database for our work\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS training\")\n",
    "spark.sql(\"USE training\")\n",
    "\n",
    "# Show current database\n",
    "print(f\"Current database: {spark.catalog.currentDatabase()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60e9371-181d-40eb-b10d-62aa0f7146dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Delta table from DataFrame\n",
    "# First, let's create some sample sales data\n",
    "from pyspark.sql.functions import col, rand, round, date_add, current_date\n",
    "\n",
    "sales_data = (\n",
    "    spark.range(1000)\n",
    "    .selectExpr(\"id as transaction_id\")\n",
    "    .withColumn(\"customer_id\", (rand() * 100).cast(\"int\"))\n",
    "    .withColumn(\"product_id\", (rand() * 50).cast(\"int\"))\n",
    "    .withColumn(\"quantity\", (rand() * 10 + 1).cast(\"int\"))\n",
    "    .withColumn(\"price\", round(rand() * 100 + 10, 2))\n",
    "    .withColumn(\"transaction_date\", date_add(current_date(), -(rand() * 30).cast(\"int\")))\n",
    ")\n",
    "\n",
    "# Write as Delta table\n",
    "delta_path = \"/tmp/delta_sales\"\n",
    "sales_data.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "print(\"Delta table created successfully!\")\n",
    "display(spark.read.format(\"delta\").load(delta_path).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f30093-3ea7-4db3-b7e0-d6f6e9196b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create managed Delta table\n",
    "sales_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_transactions\")\n",
    "\n",
    "# Query the table\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_date,\n",
    "        COUNT(*) as num_transactions,\n",
    "        SUM(quantity * price) as total_revenue\n",
    "    FROM sales_transactions\n",
    "    GROUP BY transaction_date\n",
    "    ORDER BY transaction_date DESC\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e6db50-7482-4c6c-99ed-d3b337ff74c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.4: Understanding Delta Lake Transaction Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70acf66d-ed02-484e-b3cd-1c02cb5bb599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examine Delta transaction log\n",
    "display(dbutils.fs.ls(f\"{delta_path}/_delta_log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e29e55c-84bf-4ff3-878f-6a4114c08cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read transaction log content\n",
    "import json\n",
    "\n",
    "log_file = f\"{delta_path}/_delta_log/00000000000000000000.json\"\n",
    "log_content = dbutils.fs.head(log_file, max_bytes=2000)\n",
    "\n",
    "print(\"Transaction Log Preview:\")\n",
    "print(\"=\" * 50)\n",
    "# Parse and pretty print first log entry\n",
    "for line in log_content.split('\\n')[:3]:  # First 3 entries\n",
    "    if line.strip():\n",
    "        log_entry = json.loads(line)\n",
    "        print(json.dumps(log_entry, indent=2))\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086eb6b6-0d94-4a5c-be7d-5dd2d068b2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View Delta table details\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "display(deltaTable.detail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736455ce-0531-407a-a19c-834075fa29ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.5: Delta Lake Operations and Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466144c2-aa9d-49cc-95f5-c75c9790f881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform UPDATE operation\n",
    "print(\"Before update - High value transactions:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM sales_transactions \n",
    "    WHERE quantity * price > 500 \n",
    "    LIMIT 5\n",
    "\"\"\"))\n",
    "\n",
    "# Apply 10% discount to high-value transactions\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE sales_transactions \n",
    "    SET price = price * 0.9 \n",
    "    WHERE quantity * price > 500\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAfter update - Same transactions with discount:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5936440-e3dc-48f5-9492-bc55bd2be3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add new data (simulating daily batch)\n",
    "new_sales = (\n",
    "    spark.range(100)\n",
    "    .selectExpr(\"id + 1000 as transaction_id\")\n",
    "    .withColumn(\"customer_id\", (rand() * 100).cast(\"int\"))\n",
    "    .withColumn(\"product_id\", (rand() * 50).cast(\"int\"))\n",
    "    .withColumn(\"quantity\", (rand() * 10 + 1).cast(\"int\"))\n",
    "    .withColumn(\"price\", round(rand() * 100 + 10, 2))\n",
    "    .withColumn(\"transaction_date\", current_date())\n",
    ")\n",
    "\n",
    "# Append new data\n",
    "new_sales.write.format(\"delta\").mode(\"append\").saveAsTable(\"sales_transactions\")\n",
    "\n",
    "print(f\"Total records after append: {spark.table('sales_transactions').count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e1e15a-d7eb-4758-9820-40d30bd327b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View table history\n",
    "display(spark.sql(\"DESCRIBE HISTORY sales_transactions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b0a48f0-39b6-4f2f-adc8-0a4bab389edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.6: Time Travel with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8addd2-7e7d-4a74-90df-4abd4a5b58d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get current record count\n",
    "current_count = spark.table(\"sales_transactions\").count()\n",
    "print(f\"Current record count: {current_count:,}\")\n",
    "\n",
    "# Query table as of version 0 (original load)\n",
    "version_0_count = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"sales_transactions\").count()\n",
    "print(f\"Version 0 record count: {version_0_count:,}\")\n",
    "\n",
    "# Show the difference\n",
    "print(f\"\\nRecords added since version 0: {current_count - version_0_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b204593d-b928-44d9-82fe-6cb4f80a02fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time travel using timestamp\n",
    "# Get timestamp of version 1\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY sales_transactions\")\n",
    "version_1_timestamp = history_df.filter(\"version = 1\").select(\"timestamp\").collect()[0][0]\n",
    "\n",
    "print(f\"Version 1 timestamp: {version_1_timestamp}\")\n",
    "\n",
    "# Query as of that timestamp\n",
    "df_at_timestamp = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", version_1_timestamp) \\\n",
    "    .table(\"sales_transactions\")\n",
    "\n",
    "print(f\"\\nRecords at {version_1_timestamp}: {df_at_timestamp.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c188374-e574-4484-8323-5c06d9fbb02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare data between versions\n",
    "print(\"Comparing average prices between versions:\")\n",
    "\n",
    "# Current version\n",
    "current_avg = spark.sql(\"SELECT AVG(price) as avg_price FROM sales_transactions\").collect()[0][0]\n",
    "\n",
    "# Version 0 (before discount)\n",
    "v0_avg = spark.sql(\"\"\"\n",
    "    SELECT AVG(price) as avg_price \n",
    "    FROM sales_transactions VERSION AS OF 0\n",
    "\"\"\").collect()[0][0]\n",
    "\n",
    "print(f\"Version 0 average price: ${v0_avg:.2f}\")\n",
    "print(f\"Current average price: ${current_avg:.2f}\")\n",
    "print(f\"Difference: ${v0_avg - current_avg:.2f} (due to discounts applied)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1426b0f-49f6-4189-a950-ed9566371017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.7: Delta Lake Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b888f8-ce01-421d-bbdd-8bb046bdc66f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check file statistics before optimization\n",
    "detail_df = deltaTable.detail()\n",
    "display(detail_df.select(\"numFiles\", \"sizeInBytes\", \"properties\"))\n",
    "\n",
    "# Optimize the table (compact small files)\n",
    "display(spark.sql(\"OPTIMIZE sales_transactions\"))\n",
    "\n",
    "# Check after optimization\n",
    "print(\"\\nAfter optimization:\")\n",
    "display(deltaTable.detail().select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc720c19-b9f3-4b7b-a18d-442672dba25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Z-Order optimization for better query performance\n",
    "# This co-locates related data in the same files\n",
    "display(spark.sql(\"\"\"\n",
    "    OPTIMIZE sales_transactions \n",
    "    ZORDER BY (customer_id, transaction_date)\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a0ae24-4bea-4fa9-8648-e309bb0cd246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 3 Checkpoint\n",
    "\n",
    "You've now experienced:\n",
    "- ✅ DBFS navigation and file operations\n",
    "- ✅ Reading different file formats (CSV, JSON, Parquet)\n",
    "- ✅ Creating and managing Delta tables\n",
    "- ✅ Understanding transaction logs\n",
    "- ✅ Performing updates and inserts\n",
    "- ✅ Time travel queries\n",
    "- ✅ Table optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22a5a856-3189-4005-8f23-243f8ae7ef82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 4: Collaboration and Development Workflows (30 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Use collaboration features effectively\n",
    "- Understand revision history\n",
    "- Create reusable components\n",
    "- Establish development best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c94eb4-9db0-4e85-b641-c80ad8c86394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.1: Creating Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57bd5013-fefc-43e2-b6fc-aa77b535f4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create utility functions for common operations\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql import DataFrame\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"DataEngineering\")\n",
    "\n",
    "def add_audit_columns(df: DataFrame, process_name: str = \"Unknown\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add standard audit columns to any DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - process_name: Name of the process adding these columns\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with audit columns added\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding audit columns for process: {process_name}\")\n",
    "    \n",
    "    return df.withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
    "             .withColumn(\"process_name\", lit(process_name)) \\\n",
    "             .withColumn(\"processing_cluster\", lit(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")))\n",
    "\n",
    "# Test the function\n",
    "test_df = spark.range(5)\n",
    "audited_df = add_audit_columns(test_df, \"Lab4_Testing\")\n",
    "display(audited_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412d61a8-cdb4-4fad-a447-851ee6128f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create data quality validation function\n",
    "def validate_data_quality(df: DataFrame, required_columns: list, max_null_percentage: float = 0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Validate data quality for a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to validate\n",
    "    - required_columns: List of columns that must exist\n",
    "    - max_null_percentage: Maximum allowed percentage of nulls (default 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with validation results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"row_count\": df.count(),\n",
    "        \"column_count\": len(df.columns),\n",
    "        \"missing_columns\": [],\n",
    "        \"null_percentages\": {},\n",
    "        \"validation_passed\": True\n",
    "    }\n",
    "    \n",
    "    # Check for missing columns\n",
    "    existing_columns = set(df.columns)\n",
    "    required_set = set(required_columns)\n",
    "    results[\"missing_columns\"] = list(required_set - existing_columns)\n",
    "    \n",
    "    if results[\"missing_columns\"]:\n",
    "        results[\"validation_passed\"] = False\n",
    "        logger.warning(f\"Missing required columns: {results['missing_columns']}\")\n",
    "    \n",
    "    # Check null percentages\n",
    "    total_rows = results[\"row_count\"]\n",
    "    if total_rows > 0:\n",
    "        for col_name in df.columns:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "            null_percentage = null_count / total_rows\n",
    "            results[\"null_percentages\"][col_name] = null_percentage\n",
    "            \n",
    "            if null_percentage > max_null_percentage:\n",
    "                results[\"validation_passed\"] = False\n",
    "                logger.warning(f\"Column {col_name} has {null_percentage:.2%} nulls\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test validation\n",
    "validation_results = validate_data_quality(\n",
    "    spark.table(\"sales_transactions\"),\n",
    "    required_columns=[\"transaction_id\", \"customer_id\", \"price\"],\n",
    "    max_null_percentage=0.05\n",
    ")\n",
    "\n",
    "print(\"Data Quality Validation Results:\")\n",
    "print(f\"Validation Passed: {validation_results['validation_passed']}\")\n",
    "print(f\"Row Count: {validation_results['row_count']:,}\")\n",
    "print(f\"Null Percentages: {validation_results['null_percentages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d52a99-3811-4772-ad24-b39e1512ea87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.2: Documentation Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225bbc43-826c-42bf-be11-a9c42c261a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example of well-documented data processing function\n",
    "def process_daily_sales(date_str: str, source_path: str = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process daily sales data with standardized transformations.\n",
    "    \n",
    "    This function reads raw sales data for a specific date, applies business rules,\n",
    "    and returns a cleaned DataFrame ready for analysis or storage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    date_str : str\n",
    "        Date in 'YYYY-MM-DD' format\n",
    "    source_path : str, optional\n",
    "        Override default source path for testing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Processed sales data with the following columns:\n",
    "        - transaction_id: Unique identifier\n",
    "        - customer_id: Customer identifier\n",
    "        - product_id: Product identifier\n",
    "        - quantity: Number of items\n",
    "        - unit_price: Price per item\n",
    "        - total_amount: quantity * unit_price\n",
    "        - transaction_date: Date of transaction\n",
    "        - processing_timestamp: When this record was processed\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If date_str is not in correct format\n",
    "    FileNotFoundError\n",
    "        If source data doesn't exist\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> df = process_daily_sales('2024-01-15')\n",
    "    >>> df.show(5)\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Applies 10% discount for quantities > 10\n",
    "    - Filters out test transactions (customer_id = 0)\n",
    "    - Adds audit columns for lineage tracking\n",
    "    \"\"\"\n",
    "    # Implementation would go here\n",
    "    logger.info(f\"Processing sales data for {date_str}\")\n",
    "    \n",
    "    # For demo, return sample data\n",
    "    return spark.table(\"sales_transactions\").filter(f\"transaction_date = '{date_str}'\")\n",
    "\n",
    "# Display the docstring\n",
    "help(process_daily_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5fcae6f-5092-4fe6-b7bf-0a352efa5331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.3: Development Standards Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991e0a59-300b-467c-b7a8-66f124fbfcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a development standards template\n",
    "development_standards = \"\"\"\n",
    "# Databricks Development Standards Checklist\n",
    "\n",
    "## Code Quality\n",
    "- [ ] Functions have descriptive names following snake_case convention\n",
    "- [ ] All functions include docstrings with parameters and returns documented\n",
    "- [ ] Complex logic includes inline comments\n",
    "- [ ] No hardcoded values - use configuration or parameters\n",
    "- [ ] Error handling implemented with try-except blocks\n",
    "- [ ] Logging added for key operations\n",
    "\n",
    "## Data Quality\n",
    "- [ ] Input data validated before processing\n",
    "- [ ] Null checks implemented where appropriate\n",
    "- [ ] Data types verified and cast explicitly\n",
    "- [ ] Row counts logged at each transformation step\n",
    "- [ ] Output data quality validated\n",
    "\n",
    "## Performance\n",
    "- [ ] Appropriate partitioning strategy implemented\n",
    "- [ ] Broadcast joins used for small lookup tables\n",
    "- [ ] Caching applied for repeatedly used DataFrames\n",
    "- [ ] File sizes optimized (100-200MB per file)\n",
    "- [ ] Z-ordering applied for frequently filtered columns\n",
    "\n",
    "## Security\n",
    "- [ ] No credentials hardcoded in notebooks\n",
    "- [ ] Secrets stored in secret scopes\n",
    "- [ ] Access controls reviewed and documented\n",
    "- [ ] Sensitive data columns identified and protected\n",
    "\n",
    "## Documentation\n",
    "- [ ] README created explaining the solution\n",
    "- [ ] Data flow diagram included\n",
    "- [ ] Dependencies documented\n",
    "- [ ] Runbook created for operations team\n",
    "\n",
    "## Testing\n",
    "- [ ] Unit tests for utility functions\n",
    "- [ ] Integration tests for end-to-end flow\n",
    "- [ ] Performance tests with production-scale data\n",
    "- [ ] Edge cases identified and tested\n",
    "\"\"\"\n",
    "\n",
    "# Save to a file for team reference\n",
    "dbutils.fs.put(\"/tmp/development_standards.md\", development_standards, overwrite=True)\n",
    "print(\"Development standards checklist created!\")\n",
    "print(\"\\nPreview:\")\n",
    "print(development_standards[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f046ab8-4a24-4c9a-83ae-dce5d7993c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.4: Creating a Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071ce91f-d781-402d-a5e1-42784f67108e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a summary of what we've learned\n",
    "module_summary = {\n",
    "    \"Module\": \"01 - Databricks Fundamentals\",\n",
    "    \"Topics Covered\": [\n",
    "        \"Databricks Workspace Navigation\",\n",
    "        \"Cluster Creation and Management\",\n",
    "        \"DBFS and Cloud Storage Integration\",\n",
    "        \"Delta Lake Fundamentals\",\n",
    "        \"Time Travel and Versioning\",\n",
    "        \"Collaboration Features\"\n",
    "    ],\n",
    "    \"Key Commands Learned\": [\n",
    "        \"dbutils.fs for file operations\",\n",
    "        \"spark.read/write for data I/O\",\n",
    "        \"Delta table operations\",\n",
    "        \"Time travel queries\"\n",
    "    ],\n",
    "    \"Hands-on Achievements\": [\n",
    "        \"Created and configured a cluster\",\n",
    "        \"Read multiple file formats\",\n",
    "        \"Created and optimized Delta tables\",\n",
    "        \"Performed time travel queries\",\n",
    "        \"Built reusable functions\"\n",
    "    ],\n",
    "    \"Ready for Module 2\": True\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "import json\n",
    "print(\"Module 1 Learning Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(module_summary, indent=2))\n",
    "\n",
    "# Save summary as Delta table for tracking\n",
    "summary_df = spark.createDataFrame(\n",
    "    [(\"Module_01\", \"Completed\", str(current_timestamp()))],\n",
    "    [\"module\", \"status\", \"completion_time\"]\n",
    ")\n",
    "summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"training.course_progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc83ef25-aab3-4657-bf18-7cd12575ae07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a personal reflection template\n",
    "reflection_template = f\"\"\"\n",
    "# Module 1 Reflection - {current_date()}\n",
    "\n",
    "## What I Learned\n",
    "- [Add your key learnings here]\n",
    "- \n",
    "- \n",
    "\n",
    "## What Surprised Me\n",
    "- [What was unexpected?]\n",
    "- \n",
    "\n",
    "## Questions for Follow-up\n",
    "- [What needs clarification?]\n",
    "- \n",
    "\n",
    "## How I'll Apply This\n",
    "- [Real-world applications]\n",
    "- \n",
    "\n",
    "## Technical Challenges Faced\n",
    "- [What was difficult?]\n",
    "- \n",
    "\n",
    "## Next Steps\n",
    "- Review Delta Lake documentation\n",
    "- Practice more with time travel\n",
    "- Prepare for Module 2 (ETL with PySpark)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Personal Reflection Template:\")\n",
    "print(reflection_template)\n",
    "print(\"\\n💡 Take 5 minutes to fill this out - it will help consolidate your learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cfab6dd-16c3-45f8-9b5f-055b10dd18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 4 Checkpoint\n",
    "\n",
    "You've now completed:\n",
    "- ✅ Created reusable utility functions\n",
    "- ✅ Implemented data quality validation\n",
    "- ✅ Practiced documentation standards\n",
    "- ✅ Established development best practices\n",
    "- ✅ Generated a module summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07b450d-e71d-4c69-ab47-5e4c20c906e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully completed all Module 1 laboratory exercises! \n",
    "\n",
    "### Your Achievements:\n",
    "1. **Workspace Mastery**: Navigated and organized your Databricks environment\n",
    "2. **Cluster Management**: Created and configured compute resources\n",
    "3. **Data Engineering Fundamentals**: Worked with DBFS, various file formats, and Delta Lake\n",
    "4. **Advanced Features**: Implemented time travel, optimization, and versioning\n",
    "5. **Professional Practices**: Built reusable code and documentation\n",
    "\n",
    "### Before Module 2:\n",
    "1. Review any sections that were challenging\n",
    "2. Experiment with the sample datasets\n",
    "3. Practice Delta Lake operations\n",
    "4. Prepare questions for the Wednesday session\n",
    "\n",
    "### Remember:\n",
    "- **Terminate your cluster** if you're done practicing (save costs!)\n",
    "- **Download this notebook** for future reference\n",
    "- **Join the Wednesday session** for Q&A and additional support\n",
    "\n",
    "See you in Module 2, where we'll dive deep into ETL development with PySpark! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b7e902-b0b8-40a9-8843-f79283998963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final cleanup reminder\n",
    "print(\"⚠️  IMPORTANT REMINDERS:\")\n",
    "print(\"1. Save your work - File > Save\")\n",
    "print(\"2. Export this notebook - File > Export > IPython Notebook\")\n",
    "print(\"3. Terminate your cluster - Go to Compute tab\")\n",
    "print(\"\\nGreat job today! 👏\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6851565845449433,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "module1-labs-clone-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "name": "module1_labs",
  "notebookId": "module1_labs"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
