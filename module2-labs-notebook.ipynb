{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d3a46f-1990-4678-b8cb-f08fdb0ee6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 2: ETL Development with PySpark in Databricks\n",
    "## Laboratory Exercises\n",
    "\n",
    "Welcome to the hands-on laboratory exercises for Module 2! Today we'll build real ETL pipelines for GlobalMart's data platform.\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Module 1\n",
    "- Running Databricks cluster\n",
    "- Access to sample datasets\n",
    "- Approximately 4 hours for completion\n",
    "\n",
    "### Lab Structure\n",
    "1. **Lab 1**: DataFrame Fundamentals and Basic Transformations (45 minutes)\n",
    "2. **Lab 2**: File Operations and Data Management (30 minutes)\n",
    "3. **Lab 3**: Advanced Transformations with Spark SQL (45 minutes)\n",
    "4. **Lab 4**: Delta Tables for Reliable ETL (45 minutes)\n",
    "5. **Lab 5**: Creating and Using UDFs (30 minutes)\n",
    "6. **Lab 6**: Building a Complete ETL Pipeline (60 minutes)\n",
    "\n",
    "### GlobalMart Context\n",
    "GlobalMart operates:\n",
    "- 500+ physical stores across 30 countries\n",
    "- E-commerce platform with 10M+ customers\n",
    "- 50,000+ products across multiple categories\n",
    "- Processing 1M+ transactions daily\n",
    "\n",
    "Your mission: Build the ETL pipelines that transform raw data into analytics-ready datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eec3a29-16ae-46e2-9906-17741ab36d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Run this cell first to set up your environment and create sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef96285-60ef-498e-b1ed-1fb09a31945d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Verify Spark session\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python version: {sc.pythonVer}\")\n",
    "\n",
    "# Create working directory\n",
    "working_dir = \"/tmp/module2_etl\"\n",
    "dbutils.fs.mkdirs(working_dir)\n",
    "\n",
    "# Set up database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS globalmart\")\n",
    "spark.sql(\"USE globalmart\")\n",
    "print(f\"\\nCurrent database: {spark.catalog.currentDatabase()}\")\n",
    "print(\"\\nSetup complete! Ready for ETL development.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80240fe4-fb9b-42ba-a3a1-6d7a870e1967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Let's create realistic sample data for GlobalMart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eee3595-76dd-4fb8-ae93-7d3ca5f2b068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round, rand\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "import datetime\n",
    "import random\n",
    "# Generate sample customer data with realistic quality issues\n",
    "def generate_customers(num_customers=10000):\n",
    "    \"\"\"Generate customer data with intentional quality issues\"\"\"\n",
    "    countries = ['USA', 'UK', 'Canada', 'Germany', 'France', 'Japan', 'Australia']\n",
    "    \n",
    "    customer_schema = StructType([\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"registration_date\", TimestampType(), True),\n",
    "        StructField(\"lifetime_value\", DoubleType(), True), # Changed to DoubleType\n",
    "    ])\n",
    "\n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        # Introduce data quality issues\n",
    "        email = f\"customer{i}@email.com\" if random.random() > 0.05 else None  # 5% null emails\n",
    "        phone = f\"+1-555-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}\"\n",
    "        if random.random() < 0.1:  # 10% have formatting issues\n",
    "            phone = phone.replace(\"-\", \"\") if random.random() > 0.5 else phone.replace(\"+1\", \"\")\n",
    "        \n",
    "        customers.append({\n",
    "            \"customer_id\": f\"C{str(i).zfill(6)}\",\n",
    "            \"first_name\": f\"First{i}\" if random.random() > 0.02 else None,  # 2% null names\n",
    "            \"last_name\": f\"Last{i}\",\n",
    "            \"email\": email.upper() if email and random.random() > 0.7 else email,  # Mixed case\n",
    "            \"phone\": phone,\n",
    "            \"country\": random.choice(countries),\n",
    "            \"registration_date\": (datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 1000))),\n",
    "            \"lifetime_value\": random.uniform(10, 10000) # Use random.uniform for Python list\n",
    "        })\n",
    "    \n",
    "    # Fill nulls in StringType columns with '' and DoubleType with 0.0\n",
    "    return spark.createDataFrame(customers, schema=customer_schema) \\\n",
    "                .fillna('', subset=[col.name for col in customer_schema if col.dataType == StringType()]) \\\n",
    "                .fillna(0.0, subset=[col.name for col in customer_schema if col.dataType == DoubleType()])\n",
    "\n",
    "\n",
    "# Generate sample product data\n",
    "def generate_products(num_products=1000):\n",
    "    \"\"\"Generate product catalog data\"\"\"\n",
    "    categories = ['Electronics', 'Clothing', 'Home', 'Sports', 'Books', 'Toys']\n",
    "    \n",
    "    product_schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),\n",
    "        StructField(\"cost\", DoubleType(), True),\n",
    "        StructField(\"supplier_id\", StringType(), True),\n",
    "        StructField(\"weight_kg\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    products = []\n",
    "    for i in range(num_products):\n",
    "        products.append({\n",
    "            \"product_id\": f\"P{str(i).zfill(5)}\",\n",
    "            \"product_name\": f\"Product {i}\",\n",
    "            \"category\": random.choice(categories),\n",
    "            \"price\": random.uniform(10, 500),\n",
    "            \"cost\": random.uniform(5, 250),\n",
    "            \"supplier_id\": f\"S{str(random.randint(1, 50)).zfill(3)}\",\n",
    "            \"weight_kg\": random.uniform(0.1, 20) if random.random() > 0.2 else None\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(products, schema=product_schema).fillna('')\n",
    "\n",
    "# Generate sample transactions\n",
    "def generate_transactions(num_transactions=50000):\n",
    "    \"\"\"Generate sales transaction data\"\"\"\n",
    "    channels = ['Online', 'Store', 'Mobile']\n",
    "    \n",
    "    transaction_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"channel\", StringType(), True),\n",
    "        StructField(\"transaction_date\", StringType(), True), # Keep as StringType for raw data\n",
    "        StructField(\"transaction_timestamp\", StringType(), True), # Keep as StringType for raw data\n",
    "        StructField(\"store_id\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    transactions = []\n",
    "    for i in range(num_transactions):\n",
    "        trans_date = datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 90))\n",
    "        \n",
    "        transactions.append({\n",
    "            \"transaction_id\": f\"T{str(i).zfill(8)}\",\n",
    "            \"customer_id\": f\"C{str(random.randint(0, 9999)).zfill(6)}\",\n",
    "            \"product_id\": f\"P{str(random.randint(0, 999)).zfill(5)}\",\n",
    "            \"quantity\": random.randint(1, 10),\n",
    "            \"channel\": random.choice(channels),\n",
    "            \"transaction_date\": trans_date.strftime('%Y-%m-%d'),\n",
    "            \"transaction_timestamp\": trans_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"store_id\": f\"ST{str(random.randint(1, 500)).zfill(3)}\" if random.random() > 0.5 else None # Randomly assign store_id\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(transactions, schema=transaction_schema).fillna('')\n",
    "\n",
    "# Generate all datasets\n",
    "print(\"Generating sample data...\")\n",
    "customers_df = generate_customers()\n",
    "products_df = generate_products()\n",
    "transactions_df = generate_transactions()\n",
    "\n",
    "# Save as temporary views\n",
    "customers_df.createOrReplaceTempView(\"raw_customers\")\n",
    "products_df.createOrReplaceTempView(\"raw_products\")\n",
    "transactions_df.createOrReplaceTempView(\"raw_transactions\")\n",
    "\n",
    "print(f\"✓ Generated {customers_df.count():,} customers\")\n",
    "print(f\"✓ Generated {products_df.count():,} products\")\n",
    "print(f\"✓ Generated {transactions_df.count():,} transactions\")\n",
    "print(\"\\nSample data ready for processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf8e3401-bfe8-4027-84bf-12c777c166a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 1: DataFrame Fundamentals and Basic Transformations (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Load and explore data using DataFrames\n",
    "- Apply basic transformations\n",
    "- Handle data quality issues\n",
    "- Create derived columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a90279-7f32-4e2c-824a-4a5c63ee9543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81684d64-5d29-4cc2-aee8-594b14695cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data into DataFrames\n",
    "customers = spark.table(\"raw_customers\")\n",
    "products = spark.table(\"raw_products\")\n",
    "transactions = spark.table(\"raw_transactions\")\n",
    "\n",
    "# Basic exploration\n",
    "print(\"=== CUSTOMER DATA ===\")\n",
    "print(f\"Schema:\")\n",
    "customers.printSchema()\n",
    "print(f\"\\nSample records:\")\n",
    "display(customers.limit(5))\n",
    "\n",
    "# Check for data quality issues\n",
    "print(\"\\n=== DATA QUALITY CHECK ===\")\n",
    "null_counts = customers.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in customers.columns\n",
    "])\n",
    "print(\"Null counts per column:\")\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68dc3222-96f9-43fe-adf4-91797724a316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.2: Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899b62f3-061f-49a8-b873-d75909237095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean customer data\n",
    "cleaned_customers = customers \\\n",
    "    .withColumn(\"email\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"has_email\", col(\"email\").isNotNull()) \\\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\"))) \\\n",
    "    .withColumn(\"customer_segment\", \n",
    "        when(col(\"lifetime_value\") > 5000, \"High Value\")\n",
    "        .when(col(\"lifetime_value\") > 1000, \"Medium Value\")\n",
    "        .otherwise(\"Low Value\")\n",
    "    )\n",
    "\n",
    "print(\"Cleaned customer data:\")\n",
    "display(cleaned_customers.select(\n",
    "    \"customer_id\", \"full_name\", \"email\", \"customer_segment\", \"registration_year\"\n",
    ").limit(10))\n",
    "\n",
    "# Analyze customer segments\n",
    "print(\"\\nCustomer Segmentation:\")\n",
    "segment_analysis = cleaned_customers \\\n",
    "    .groupBy(\"customer_segment\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"lifetime_value\").alias(\"avg_lifetime_value\"),\n",
    "        min(\"registration_date\").alias(\"earliest_registration\")\n",
    "    ) \\\n",
    "    .orderBy(\"avg_lifetime_value\", ascending=False)\n",
    "\n",
    "display(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595f1fc9-716e-492e-a250-f655df9f123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.3: Working with Multiple DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2399c9-3636-4c3b-be04-1e2a63a4a794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join transactions with product information\n",
    "enriched_transactions = transactions \\\n",
    "    .join(products, \"product_id\", \"left\") \\\n",
    "    .withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .withColumn(\"profit\", col(\"revenue\") - (col(\"quantity\") * col(\"cost\")))\n",
    "\n",
    "# Daily sales summary\n",
    "daily_sales = enriched_transactions \\\n",
    "    .groupBy(\"transaction_date\", \"channel\") \\\n",
    "    .agg(\n",
    "        count(\"transaction_id\").alias(\"num_transactions\"),\n",
    "        sum(\"quantity\").alias(\"units_sold\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        sum(\"profit\").alias(\"total_profit\"),\n",
    "        avg(\"revenue\").alias(\"avg_transaction_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"transaction_date\", \"channel\")\n",
    "\n",
    "print(\"Daily Sales Summary (last 7 days):\")\n",
    "display(daily_sales.filter(\n",
    "    col(\"transaction_date\") >= date_sub(current_date(), 7)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0b6b10-f600-4061-b09d-74857abbe30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.4: Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117f3483-789e-4208-b1e6-865e50322c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create data quality validation function\n",
    "def validate_dataframe(df, validations):\n",
    "    \"\"\"\n",
    "    Validate DataFrame based on rules\n",
    "    validations: dict of column_name -> validation_function\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    for column, validation_expr in validations.items():\n",
    "        valid_count = df.filter(validation_expr).count()\n",
    "        invalid_count = total_rows - valid_count\n",
    "        \n",
    "        results[column] = {\n",
    "            \"total\": total_rows,\n",
    "            \"valid\": valid_count,\n",
    "            \"invalid\": invalid_count,\n",
    "            \"validity_rate\": valid_count / total_rows * 100\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define validation rules\n",
    "customer_validations = {\n",
    "    \"email\": col(\"email\").isNotNull() & col(\"email\").contains(\"@\"),\n",
    "    \"phone\": col(\"phone\").isNotNull() & (length(col(\"phone\")) >= 10),\n",
    "    \"lifetime_value\": col(\"lifetime_value\").isNotNull() & (col(\"lifetime_value\") >= 0),\n",
    "    \"registration_date\": col(\"registration_date\").isNotNull()\n",
    "}\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_dataframe(customers, customer_validations)\n",
    "\n",
    "print(\"Customer Data Quality Report:\")\n",
    "print(\"=\" * 50)\n",
    "for column, stats in validation_results.items():\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"  - Valid: {stats['valid']:,} ({stats['validity_rate']}%)\")\n",
    "    print(f\"  - Invalid: {stats['invalid']:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5642fb66-178b-4078-8026-420c6b6ebff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 1 Key Takeaways\n",
    "- DataFrames are the foundation of ETL in Spark\n",
    "- Transformations are lazy - nothing executes until an action\n",
    "- Always validate data quality early in your pipeline\n",
    "- Use built-in functions for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f38b3a-e67f-499a-b77e-61d425f14b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 2: File Operations and Data Management (30 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Master dbutils.fs operations\n",
    "- Implement file-based processing patterns\n",
    "- Handle multiple file formats\n",
    "- Archive processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78373616-2358-48fe-ab85-38b022ba8ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.1: File System Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf6253e-c4c7-41e4-a6ab-71d669726c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "base_path = \"/tmp/module2_etl\"\n",
    "paths = {\n",
    "    \"landing\": f\"{base_path}/landing\",\n",
    "    \"processing\": f\"{base_path}/processing\",\n",
    "    \"processed\": f\"{base_path}/processed\",\n",
    "    \"error\": f\"{base_path}/error\",\n",
    "    \"archive\": f\"{base_path}/archive\"\n",
    "}\n",
    "\n",
    "# Create all directories\n",
    "for name, path in paths.items():\n",
    "    dbutils.fs.mkdirs(path)\n",
    "    print(f\"Created: {path}\")\n",
    "\n",
    "# List directory contents\n",
    "print(\"\\nDirectory structure:\")\n",
    "display(dbutils.fs.ls(base_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444ff50a-5093-4a75-8aee-e93087ae87d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.2: Writing Data in Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30be7b7a-fd4f-434c-ab9d-778ac6b35274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare sample data for export\n",
    "export_data = cleaned_customers.limit(1000)\n",
    "\n",
    "# Write in different formats\n",
    "formats = {\n",
    "    \"csv\": {\"path\": f\"{paths['landing']}/customers.csv\", \"options\": {\"header\": \"true\"}},\n",
    "    \"json\": {\"path\": f\"{paths['landing']}/customers.json\", \"options\": {}},\n",
    "    \"parquet\": {\"path\": f\"{paths['landing']}/customers.parquet\", \"options\": {}}\n",
    "}\n",
    "\n",
    "for format_name, config in formats.items():\n",
    "    export_data.coalesce(1).write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .options(**config[\"options\"]) \\\n",
    "        .format(format_name) \\\n",
    "        .save(config[\"path\"])\n",
    "    print(f\"✓ Wrote {format_name} to {config['path']}\")\n",
    "\n",
    "# Check file sizes\n",
    "print(\"\\nFile sizes comparison:\")\n",
    "for format_name, config in formats.items():\n",
    "    files = dbutils.fs.ls(config[\"path\"])\n",
    "    data_files = [f for f in files if not f.name.startswith(\"_\")]\n",
    "    if data_files:\n",
    "        size_mb = data_files[0].size / 1024 / 1024\n",
    "        print(f\"{format_name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b4d76b-7aeb-47fb-99ea-6ae927c3a60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.3: File Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9e6d96-9986-4856-af41-8c6751984747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate incoming files\n",
    "for i in range(3):\n",
    "    file_data = transactions.limit(100).withColumn(\"batch_id\", lit(i))\n",
    "    file_path = f\"{paths['landing']}/transactions_batch_{i}.csv\"\n",
    "    \n",
    "    file_data.coalesce(1).write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    print(f\"Created: {file_path}\")\n",
    "\n",
    "# Process files\n",
    "def process_file(file_path, archive_path):\n",
    "    \"\"\"\n",
    "    Process a single file and archive it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read file\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Process (simple transformation)\n",
    "        processed_df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "        \n",
    "        # Move to archive\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        archive_file = f\"{archive_path}/{timestamp}_{file_path.split('/')[-1]}\"\n",
    "        dbutils.fs.mv(file_path, archive_file,recurse=True)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"records\": record_count,\n",
    "            \"archived_to\": archive_file\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Process all files in landing\n",
    "landing_files = dbutils.fs.ls(paths['landing'])\n",
    "csv_files = [f.path for f in landing_files if ('csv' in f.path) and ('transactions' in f.path)]\n",
    "\n",
    "print(\"Processing files:\")\n",
    "for file_path in csv_files:\n",
    "    result = process_file(file_path, paths['archive'])\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d388777d-6738-437e-84c5-6c0ad9a19eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.4: File Monitoring Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53a4e69-eadd-4b4e-8718-13cd9acaecfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create file monitoring function\n",
    "def monitor_directory(path, pattern=None):\n",
    "    \"\"\"\n",
    "    Monitor directory for files matching pattern\n",
    "    \"\"\"\n",
    "    files = dbutils.fs.ls(path)\n",
    "    \n",
    "    # Filter by pattern if provided\n",
    "    if pattern:\n",
    "        files = [f for f in files if pattern in f.name]\n",
    "    \n",
    "    # Get file details\n",
    "    file_info = []\n",
    "    for file in files:\n",
    "        if not file.name.startswith(\"_\"):  # Skip metadata files\n",
    "            file_info.append({\n",
    "                \"name\": file.name,\n",
    "                \"path\": file.path,\n",
    "                \"size\": file.size,\n",
    "                \"modified_time\": datetime.datetime.fromtimestamp(file.modificationTime / 1000)\n",
    "            })\n",
    "    \n",
    "    return sorted(file_info, key=lambda x: x['modified_time'], reverse=True)\n",
    "\n",
    "# Monitor archive directory\n",
    "print(\"Archive Directory Contents:\")\n",
    "archived_files = monitor_directory(paths['archive'])\n",
    "\n",
    "for file in archived_files:\n",
    "    print(f\"\\nFile: {file['name']}\")\n",
    "    print(f\"  Size: {file['size']} B\")\n",
    "    print(f\"  Modified: {file['modified_time']}\")\n",
    "\n",
    "# Create processing log\n",
    "processing_log = spark.createDataFrame(archived_files)\n",
    "processing_log.write.mode(\"overwrite\").json(f\"{paths['processed']}/processing_log\")\n",
    "print(\"\\n✓ Processing log saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5230036-7c69-4184-b546-c4ef285cefbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 2 Key Takeaways\n",
    "- dbutils.fs provides powerful file operations\n",
    "- Always implement proper file archiving\n",
    "- Monitor directories for new files to process\n",
    "- Different formats have different size/performance characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e081ef22-57ec-4e3e-b828-734dbe27d28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 3: Advanced Transformations with Spark SQL (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Use Spark SQL for complex transformations\n",
    "- Combine SQL and DataFrame operations\n",
    "- Implement window functions\n",
    "- Optimize query performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f3c42cd-d2e7-407a-b473-5f18dc34c75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.1: Creating and Using Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe3a0b5-21c5-496b-a1c2-59828cce6aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary views\n",
    "cleaned_customers.createOrReplaceTempView(\"customers\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "enriched_transactions.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# Show available tables\n",
    "print(\"Available tables:\")\n",
    "display(spark.sql(\"SHOW TABLES\"))\n",
    "\n",
    "# Basic SQL query\n",
    "top_customers_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        full_name,\n",
    "        customer_segment,\n",
    "        lifetime_value,\n",
    "        registration_date\n",
    "    FROM customers\n",
    "    WHERE lifetime_value > 5000\n",
    "    ORDER BY lifetime_value DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTop 10 High-Value Customers:\")\n",
    "display(top_customers_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d107a8dd-58bc-4007-85d4-833265989579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.2: Complex Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1272f3ca-6703-4f00-b1b3-e07195b77da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monthly sales analysis with multiple metrics\n",
    "monthly_analysis = spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', transaction_date) as month,\n",
    "            channel,\n",
    "            category,\n",
    "            COUNT(DISTINCT t.customer_id) as unique_customers,\n",
    "            COUNT(*) as transaction_count,\n",
    "            SUM(quantity) as units_sold,\n",
    "            SUM(revenue) as total_revenue,\n",
    "            SUM(profit) as total_profit,\n",
    "            AVG(revenue) as avg_transaction_value\n",
    "        FROM transactions t\n",
    "        GROUP BY DATE_TRUNC('month', transaction_date), channel, category\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        channel,\n",
    "        category,\n",
    "        unique_customers,\n",
    "        transaction_count,\n",
    "        units_sold,\n",
    "        ROUND(total_revenue, 2) as total_revenue,\n",
    "        ROUND(total_profit, 2) as total_profit,\n",
    "        ROUND(avg_transaction_value, 2) as avg_transaction_value,\n",
    "        ROUND(total_profit / total_revenue * 100, 2) as profit_margin_pct\n",
    "    FROM monthly_sales\n",
    "    WHERE month >= DATE_SUB(CURRENT_DATE(), 90)\n",
    "    ORDER BY month DESC, total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Monthly Sales Analysis:\")\n",
    "display(monthly_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1733c6d-ea73-4c67-be0c-664e783b32d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.3: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7ff53e-2efb-44bb-9441-5926e656f017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Customer purchase patterns with window functions\n",
    "customer_patterns = spark.sql(\"\"\"\n",
    "    WITH customer_transactions AS (\n",
    "        SELECT \n",
    "            t.customer_id,\n",
    "            c.full_name,\n",
    "            c.customer_segment,\n",
    "            t.transaction_date,\n",
    "            t.revenue,\n",
    "            -- Running total per customer\n",
    "            SUM(t.revenue) OVER (\n",
    "                PARTITION BY t.customer_id \n",
    "                ORDER BY t.transaction_date \n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "            ) as running_total,\n",
    "            -- Days since last purchase\n",
    "            DATEDIFF(\n",
    "                t.transaction_date,\n",
    "                LAG(t.transaction_date, 1) OVER (\n",
    "                    PARTITION BY t.customer_id \n",
    "                    ORDER BY t.transaction_date\n",
    "                )\n",
    "            ) as days_since_last_purchase,\n",
    "            -- Customer's transaction rank\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY t.customer_id \n",
    "                ORDER BY t.transaction_date\n",
    "            ) as transaction_number,\n",
    "            -- Percentile rank by revenue\n",
    "            PERCENT_RANK() OVER (\n",
    "                PARTITION BY t.customer_id \n",
    "                ORDER BY t.revenue\n",
    "            ) as revenue_percentile\n",
    "        FROM transactions t\n",
    "        JOIN customers c ON t.customer_id = c.customer_id\n",
    "    )\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        full_name,\n",
    "        customer_segment,\n",
    "        transaction_date,\n",
    "        revenue,\n",
    "        running_total,\n",
    "        days_since_last_purchase,\n",
    "        transaction_number,\n",
    "        ROUND(revenue_percentile * 100, 2) as revenue_percentile_pct\n",
    "    FROM customer_transactions\n",
    "    WHERE transaction_number <= 5  -- First 5 transactions per customer\n",
    "    ORDER BY customer_id, transaction_date\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"Customer Purchase Patterns (Window Functions):\")\n",
    "display(customer_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4325033a-c073-4c34-80dd-92daa2924935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.4: Combining SQL and DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc0266d-9e21-47a7-b171-94e0a4dbec39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start with SQL for complex logic\n",
    "cohort_analysis_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', c.registration_date) as cohort_month,\n",
    "        c.customer_segment,\n",
    "        COUNT(DISTINCT c.customer_id) as cohort_size,\n",
    "        COUNT(DISTINCT t.customer_id) as active_customers,\n",
    "        SUM(t.revenue) as cohort_revenue\n",
    "    FROM customers c\n",
    "    LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    WHERE c.registration_date >= DATE_SUB(CURRENT_DATE(), 180)\n",
    "    GROUP BY DATE_TRUNC('month', c.registration_date), c.customer_segment\n",
    "\"\"\")\n",
    "\n",
    "# Continue with DataFrame API for additional processing\n",
    "cohort_analysis_df = cohort_analysis_sql \\\n",
    "    .withColumn(\"activation_rate\", \n",
    "        col(\"active_customers\") / col(\"cohort_size\") * 100) \\\n",
    "    .withColumn(\"avg_revenue_per_active\", \n",
    "        when(col(\"active_customers\") > 0, \n",
    "             col(\"cohort_revenue\") / col(\"active_customers\"))\n",
    "        .otherwise(0)) \\\n",
    "    .orderBy(\"cohort_month\", \"customer_segment\")\n",
    "\n",
    "print(\"Cohort Analysis (SQL + DataFrame):\")\n",
    "display(cohort_analysis_df)\n",
    "\n",
    "# Create a pivot table for better visualization\n",
    "cohort_pivot = cohort_analysis_df \\\n",
    "    .groupBy(\"cohort_month\") \\\n",
    "    .pivot(\"customer_segment\") \\\n",
    "    .agg(first(\"activation_rate\")) \\\n",
    "    .orderBy(\"cohort_month\")\n",
    "\n",
    "print(\"\\nActivation Rate by Cohort and Segment:\")\n",
    "display(cohort_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4daff99f-9a03-46dc-836a-eefa942a22e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 3 Key Takeaways\n",
    "- Spark SQL provides familiar syntax for complex transformations\n",
    "- Window functions enable sophisticated analytics\n",
    "- Combine SQL and DataFrame API for maximum flexibility\n",
    "- CTEs (WITH clauses) improve query readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d658c93e-35f3-4978-9b47-b1f75b9fe35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 4: Delta Tables for Reliable ETL (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Create and manage Delta tables\n",
    "- Implement merge operations\n",
    "- Use time travel for data recovery\n",
    "- Optimize Delta table performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc7a41b-934a-4e95-9700-43c81dfaf694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.1: Creating Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ceebcb-6d91-4bcb-ac95-c4cc6dc6ce33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create managed Delta tables\n",
    "# Customer dimension table\n",
    "cleaned_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"globalmart.dim_customers\")\n",
    "\n",
    "print(\"✓ Created dim_customers table\")\n",
    "\n",
    "# Product dimension table\n",
    "products.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"globalmart.dim_products\")\n",
    "\n",
    "print(\"✓ Created dim_products table\")\n",
    "\n",
    "# Transaction fact table - partitioned by date\n",
    "enriched_transactions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"globalmart.fact_transactions\")\n",
    "\n",
    "print(\"✓ Created fact_transactions table\")\n",
    "\n",
    "# Verify tables\n",
    "print(\"\\nDelta Tables:\")\n",
    "display(spark.sql(\"SHOW TABLES IN globalmart\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a047a6-d52a-4e85-a97f-14d791d97f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.2: Delta Merge Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406c68f6-005a-4800-9828-a53ce498cea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate customer updates\n",
    "customer_updates = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        LOWER(email) as email,\n",
    "        phone,\n",
    "        country,\n",
    "        registration_date,\n",
    "        CASE \n",
    "            WHEN customer_id LIKE '%00' THEN lifetime_value * 1.1  -- 10% increase\n",
    "            ELSE lifetime_value \n",
    "        END as lifetime_value,\n",
    "        full_name,\n",
    "        has_email,\n",
    "        registration_year,\n",
    "        CASE \n",
    "            WHEN lifetime_value * 1.1 > 5000 THEN 'High Value'\n",
    "            WHEN lifetime_value * 1.1 > 1000 THEN 'Medium Value'\n",
    "            ELSE 'Low Value'\n",
    "        END as customer_segment\n",
    "    FROM globalmart.dim_customers\n",
    "    WHERE customer_id LIKE '%0'  -- Update 10% of customers\n",
    "\"\"\")\n",
    "\n",
    "# New customers to insert\n",
    "new_customers = generate_customers(100) \\\n",
    "    .withColumn(\"customer_id\", concat(lit(\"N\"), col(\"customer_id\"))) \\\n",
    "    .withColumn(\"email\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"has_email\", col(\"email\").isNotNull()) \\\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\"))) \\\n",
    "    .withColumn(\"customer_segment\", \n",
    "        when(col(\"lifetime_value\") > 5000, \"High Value\")\n",
    "        .when(col(\"lifetime_value\") > 1000, \"Medium Value\")\n",
    "        .otherwise(\"Low Value\")\n",
    "    )\n",
    "\n",
    "# Combine updates and inserts\n",
    "merge_data = customer_updates.union(new_customers)\n",
    "\n",
    "print(f\"Updates: {customer_updates.count()}\")\n",
    "print(f\"New: {new_customers.count()}\")\n",
    "print(f\"Total merge records: {merge_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd9c2cb-ce43-4583-9525-aac42a759cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform merge operation\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Get Delta table reference\n",
    "customer_delta = DeltaTable.forName(spark, \"globalmart.dim_customers\")\n",
    "\n",
    "# Record counts before merge\n",
    "before_count = spark.table(\"globalmart.dim_customers\").count()\n",
    "\n",
    "# Perform merge\n",
    "merge_result = customer_delta.alias(\"target\").merge(\n",
    "    merge_data.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"lifetime_value\": \"source.lifetime_value\",\n",
    "        \"customer_segment\": \"source.customer_segment\",\n",
    "        \"email\": \"source.email\"  # Update email in case it changed\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"first_name\": \"source.first_name\",\n",
    "        \"last_name\": \"source.last_name\",\n",
    "        \"email\": \"source.email\",\n",
    "        \"phone\": \"source.phone\",\n",
    "        \"country\": \"source.country\",\n",
    "        \"registration_date\": \"source.registration_date\",\n",
    "        \"lifetime_value\": \"source.lifetime_value\",\n",
    "        \"full_name\": \"source.full_name\",\n",
    "        \"has_email\": \"source.has_email\",\n",
    "        \"registration_year\": \"source.registration_year\",\n",
    "        \"customer_segment\": \"source.customer_segment\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "# Record counts after merge\n",
    "after_count = spark.table(\"globalmart.dim_customers\").count()\n",
    "\n",
    "print(f\"\\nMerge Results:\")\n",
    "print(f\"Records before: {before_count:,}\")\n",
    "print(f\"Records after: {after_count:,}\")\n",
    "print(f\"Net new records: {after_count - before_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ada29cc8-2f45-431b-8541-a7ee7b501e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.3: Time Travel and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e05ff62-ad1e-4518-acf1-793c59bdbc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View table history\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY globalmart.dim_customers\")\n",
    "print(\"Table History:\")\n",
    "display(history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))\n",
    "\n",
    "# Get version numbers\n",
    "versions = history_df.select(\"version\").collect()\n",
    "latest_version = versions[0][0]\n",
    "previous_version = versions[1][0] if len(versions) > 1 else 0\n",
    "\n",
    "print(f\"\\nLatest version: {latest_version}\")\n",
    "print(f\"Previous version: {previous_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4b9ccb-22ab-4d22-a347-1db7fe1f4ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare versions using time travel\n",
    "if previous_version != latest_version:\n",
    "    # Read previous version\n",
    "    customers_previous = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"versionAsOf\", previous_version) \\\n",
    "        .table(\"globalmart.dim_customers\")\n",
    "    \n",
    "    # Read current version\n",
    "    customers_current = spark.table(\"globalmart.dim_customers\")\n",
    "    \n",
    "    # Find changes\n",
    "    # New customers\n",
    "    new_customers = customers_current.join(\n",
    "        customers_previous,\n",
    "        customers_current.customer_id == customers_previous.customer_id,\n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Updated customers\n",
    "    updated_customers = customers_current.alias(\"curr\").join(\n",
    "        customers_previous.alias(\"prev\"),\n",
    "        col(\"curr.customer_id\") == col(\"prev.customer_id\"),\n",
    "        \"inner\"\n",
    "    ).filter(\n",
    "        col(\"curr.lifetime_value\") != col(\"prev.lifetime_value\")\n",
    "    ).select(\n",
    "        col(\"curr.customer_id\"),\n",
    "        col(\"curr.full_name\"),\n",
    "        col(\"prev.lifetime_value\").alias(\"old_lifetime_value\"),\n",
    "        col(\"curr.lifetime_value\").alias(\"new_lifetime_value\"),\n",
    "        (col(\"curr.lifetime_value\") - col(\"prev.lifetime_value\")).alias(\"change\")\n",
    "    )\n",
    "    \n",
    "    print(f\"New customers: {new_customers.count()}\")\n",
    "    print(f\"Updated customers: {updated_customers.count()}\")\n",
    "    \n",
    "    print(\"\\nSample of updated customers:\")\n",
    "    display(updated_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8453a78-13f2-4b19-a495-6cde53b8fc36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.4: Delta Table Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63c827e-6190-4acb-8bbc-57df281e2c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table details before optimization\n",
    "detail_before = spark.sql(\"DESCRIBE DETAIL globalmart.fact_transactions\")\n",
    "print(\"Table Details Before Optimization:\")\n",
    "display(detail_before.select(\"numFiles\", \"sizeInBytes\", \"properties\"))\n",
    "\n",
    "# Optimize table\n",
    "print(\"\\nRunning OPTIMIZE...\")\n",
    "optimize_result = spark.sql(\"\"\"\n",
    "    OPTIMIZE globalmart.fact_transactions\n",
    "    ZORDER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)\n",
    "\n",
    "# Check table details after optimization\n",
    "detail_after = spark.sql(\"DESCRIBE DETAIL globalmart.fact_transactions\")\n",
    "print(\"\\nTable Details After Optimization:\")\n",
    "display(detail_after.select(\"numFiles\", \"sizeInBytes\", \"properties\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe01818-f542-454d-8ae2-d0ed4683fd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 4 Key Takeaways\n",
    "- Delta tables provide ACID transactions on data lakes\n",
    "- Merge operations enable efficient upserts\n",
    "- Time travel allows data recovery and auditing\n",
    "- Optimization improves query performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8acbdd2a-5cd6-4065-b709-f6680aeb935c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 5: Creating and Using UDFs (30 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Create custom UDFs for business logic\n",
    "- Handle edge cases in UDFs\n",
    "- Compare UDF performance\n",
    "- Implement Pandas UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e78b7b0-ec44-4e24-9629-4aa14b02c2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.1: Creating Basic UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db05fd85-8d80-4c33-b996-deecefa34456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, least, lit\n",
    "from pyspark.sql.types import StringType, FloatType, BooleanType\n",
    "import re\n",
    "\n",
    "# Email validation UDF\n",
    "def validate_email(email):\n",
    "    \"\"\"Validate email format\"\"\"\n",
    "    if email is None:\n",
    "        return False\n",
    "    \n",
    "    # Simple email regex\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, email.lower()))\n",
    "\n",
    "# Register UDF\n",
    "validate_email_udf = udf(validate_email, BooleanType())\n",
    "\n",
    "# Phone formatting UDF\n",
    "def format_phone(phone):\n",
    "    \"\"\"Format phone numbers consistently\"\"\"\n",
    "    if phone is None:\n",
    "        return None\n",
    "    \n",
    "    # Remove all non-digits\n",
    "    digits = re.sub(r'\\D', '', phone)\n",
    "    \n",
    "    # Format as (XXX) XXX-XXXX for 10 digits\n",
    "    if len(digits) == 10:\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\"\n",
    "    elif len(digits) == 11 and digits[0] == '1':\n",
    "        return f\"+1 ({digits[1:4]}) {digits[4:7]}-{digits[7:]}\"\n",
    "    else:\n",
    "        return phone  # Return original if can't format\n",
    "\n",
    "format_phone_udf = udf(format_phone, StringType())\n",
    "\n",
    "# Customer score UDF (complex business logic)\n",
    "def calculate_customer_score(lifetime_value, registration_date, transaction_count):\n",
    "    \"\"\"Calculate customer score based on multiple factors\"\"\"\n",
    "    if lifetime_value is None or registration_date is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Days since registration\n",
    "    days_active = (datetime.datetime.now().date() - registration_date.date()).days\n",
    "    \n",
    "    # Score components\n",
    "    value_score = lifetime_value / 1000 if lifetime_value / 1000 < 10 else 10\n",
    "    tenure_score = days_active / 365 if days_active / 365 < 5 else 5\n",
    "    frequency_score = 0 if transaction_count is None else (transaction_count / 10 if transaction_count / 10 < 5 else 5)\n",
    "    \n",
    "    total_score = value_score + tenure_score + frequency_score\n",
    "    return float(total_score)\n",
    "\n",
    "calculate_score_udf = udf(calculate_customer_score, FloatType())\n",
    "\n",
    "print(\"✓ UDFs created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "898a70d1-5604-4ec9-9393-36c82950283a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.2: Applying UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9814d0-287e-414a-890e-4311e36e6d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer data and add transaction counts\n",
    "customer_metrics = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        c.*,\n",
    "        COALESCE(t.transaction_count, 0) as transaction_count\n",
    "    FROM globalmart.dim_customers c\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(*) as transaction_count\n",
    "        FROM globalmart.fact_transactions\n",
    "        GROUP BY customer_id\n",
    "    ) t ON c.customer_id = t.customer_id\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Apply UDFs\n",
    "customers_enhanced = (\n",
    "    customer_metrics.withColumn(\"email_valid\", validate_email_udf(col(\"email\")))\n",
    "    .withColumn(\"phone_formatted\", format_phone_udf(col(\"phone\")))\n",
    "    .withColumn(\n",
    "        \"customer_score\",\n",
    "        calculate_score_udf(\n",
    "            col(\"lifetime_value\"), col(\"registration_date\"), col(\"transaction_count\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"Enhanced Customer Data:\")\n",
    "display(\n",
    "    customers_enhanced.select(\n",
    "        \"customer_id\",\n",
    "        \"email\",\n",
    "        \"email_valid\",\n",
    "        \"phone\",\n",
    "        \"phone_formatted\",\n",
    "        \"lifetime_value\",\n",
    "        \"transaction_count\",\n",
    "        \"customer_score\",\n",
    "    ).limit(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773589ba-65da-477e-952c-c87a2bc16295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de494c29-c69d-46df-84f5-cd151a5c7ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_enhanced = customer_metrics \\\n",
    "    .withColumn(\"email_valid\", validate_email_udf(col(\"email\"))) \\\n",
    "    .withColumn(\"phone_formatted\", format_phone_udf(col(\"phone\"))) \\\n",
    "    .withColumn(\"customer_score\", \n",
    "        calculate_score_udf(\n",
    "            col(\"lifetime_value\"),\n",
    "            col(\"registration_date\"),\n",
    "            col(\"transaction_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "customers_enhanced.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65a651c-1f89-4431-af8e-9455a6f58ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.3: Pandas UDFs for Better Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cabb2a-bcab-44c2-bf87-51c4b6098348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Pandas UDF for customer scoring (vectorized)\n",
    "@pandas_udf(returnType=FloatType())\n",
    "def calculate_customer_score_pandas(lifetime_value: pd.Series, \n",
    "                                   registration_date: pd.Series,\n",
    "                                   transaction_count: pd.Series) -> pd.Series:\n",
    "    \"\"\"Vectorized customer score calculation\"\"\"\n",
    "    # Calculate days active\n",
    "    today = pd.Timestamp.now().date()\n",
    "    days_active = (today - registration_date.dt.date).dt.days\n",
    "    \n",
    "    # Score components\n",
    "    value_score = (lifetime_value / 1000).clip(upper=10)\n",
    "    tenure_score = (days_active / 365).clip(upper=5)\n",
    "    frequency_score = (transaction_count / 10).clip(upper=5)\n",
    "    \n",
    "    # Total score\n",
    "    total_score = value_score + tenure_score + frequency_score\n",
    "    \n",
    "    # Handle nulls\n",
    "    total_score = total_score.fillna(0.0).round(2)\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# Performance comparison\n",
    "import time\n",
    "\n",
    "# Test with regular UDF\n",
    "start_time = time.time()\n",
    "regular_udf_result = customer_metrics \\\n",
    "    .withColumn(\"score_regular\", \n",
    "        calculate_score_udf(\n",
    "            col(\"lifetime_value\"),\n",
    "            col(\"registration_date\"),\n",
    "            col(\"transaction_count\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(avg(\"score_regular\")).collect()[0][0]\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "# Test with Pandas UDF\n",
    "start_time = time.time()\n",
    "pandas_udf_result = customer_metrics \\\n",
    "    .withColumn(\"score_pandas\",\n",
    "        calculate_customer_score_pandas(\n",
    "            col(\"lifetime_value\"),\n",
    "            col(\"registration_date\"),\n",
    "            col(\"transaction_count\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(avg(\"score_pandas\")).collect()[0][0]\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Regular UDF: {regular_time:.2f} seconds (avg score: {regular_udf_result:.2f})\")\n",
    "print(f\"Pandas UDF: {pandas_time:.2f} seconds (avg score: {pandas_udf_result:.2f})\")\n",
    "print(f\"Speedup: {regular_time/pandas_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99a3506-efbc-4e6b-9899-ca4e21267920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.4: SQL UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe712063-2d2f-4986-a135-83d483a20147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register Python UDF for SQL use\n",
    "spark.udf.register(\"validate_email_sql\", validate_email, BooleanType())\n",
    "spark.udf.register(\"format_phone_sql\", format_phone, StringType())\n",
    "\n",
    "# Use UDF in SQL\n",
    "sql_udf_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        email,\n",
    "        validate_email_sql(email) as is_valid_email,\n",
    "        phone,\n",
    "        format_phone_sql(phone) as formatted_phone,\n",
    "        CASE \n",
    "            WHEN validate_email_sql(email) = true THEN 'Valid'\n",
    "            ELSE 'Invalid'\n",
    "        END as email_status\n",
    "    FROM globalmart.dim_customers\n",
    "    WHERE customer_id LIKE 'C00000%'\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL UDF Results:\")\n",
    "display(sql_udf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f8bf57-3212-44eb-994a-4a30b639bd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 5 Key Takeaways\n",
    "- UDFs enable custom business logic in Spark\n",
    "- Always handle null values in UDFs\n",
    "- Pandas UDFs offer better performance for complex operations\n",
    "- Register UDFs for SQL use when needed\n",
    "- Use built-in functions when possible for best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41835ab-ebef-43f7-a6d5-c772b04261c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 6: Building a Complete ETL Pipeline (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Integrate all components into a production-ready pipeline\n",
    "- Implement comprehensive error handling\n",
    "- Add monitoring and logging\n",
    "- Optimize end-to-end performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9ddedb-efdf-4fed-bff0-7cc3bc8a426b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 6.1: Pipeline Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd2f3cf-1a30-4d68-acdf-b4c9d7ab1f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline framework\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"GlobalMartETL\")\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"\n",
    "    Complete ETL Pipeline for GlobalMart\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark, config):\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.metrics = {}\n",
    "        self.start_time = None\n",
    "        \n",
    "    def log_metric(self, metric_name, value):\n",
    "        \"\"\"Log pipeline metrics\"\"\"\n",
    "        self.metrics[metric_name] = value\n",
    "        logger.info(f\"Metric - {metric_name}: {value}\")\n",
    "    \n",
    "    def validate_data(self, df, stage_name, required_columns):\n",
    "        \"\"\"Validate data at each stage\"\"\"\n",
    "        logger.info(f\"Validating {stage_name}...\")\n",
    "        \n",
    "        # Check record count\n",
    "        count = df.count()\n",
    "        self.log_metric(f\"{stage_name}_count\", count)\n",
    "        \n",
    "        if count == 0:\n",
    "            raise ValueError(f\"No records found in {stage_name}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        missing_cols = set(required_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in {stage_name}: {missing_cols}\")\n",
    "        \n",
    "        # Check for nulls in key columns\n",
    "        null_counts = df.select([\n",
    "            sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "            for c in required_columns\n",
    "        ]).collect()[0]\n",
    "        \n",
    "        for col_name, null_count in null_counts.asDict().items():\n",
    "            if null_count > count * 0.1:  # More than 10% nulls\n",
    "                logger.warning(f\"High null rate in {stage_name}.{col_name}: {null_count/count:.2%}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def extract_data(self):\n",
    "        \"\"\"Extract data from sources\"\"\"\n",
    "        logger.info(\"Starting data extraction...\")\n",
    "        \n",
    "        try:\n",
    "            # In real scenario, this would read from external sources\n",
    "            customers = self.spark.table(\"raw_customers\")\n",
    "            products = self.spark.table(\"raw_products\")\n",
    "            transactions = self.spark.table(\"raw_transactions\")\n",
    "            \n",
    "            # Validate extracted data\n",
    "            self.validate_data(customers, \"customers\", [\"customer_id\", \"registration_date\"])\n",
    "            self.validate_data(products, \"products\", [\"product_id\", \"price\"])\n",
    "            self.validate_data(transactions, \"transactions\", [\"transaction_id\", \"customer_id\", \"product_id\"])\n",
    "            \n",
    "            return {\n",
    "                \"customers\": customers,\n",
    "                \"products\": products,\n",
    "                \"transactions\": transactions\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Extraction failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def transform_data(self, raw_data):\n",
    "        \"\"\"Transform data with business logic\"\"\"\n",
    "        logger.info(\"Starting data transformation...\")\n",
    "        \n",
    "        try:\n",
    "            # Customer transformations\n",
    "            customers_clean = raw_data[\"customers\"] \\\n",
    "                .withColumn(\"email\", lower(trim(col(\"email\")))) \\\n",
    "                .withColumn(\"email_valid\", validate_email_udf(col(\"email\"))) \\\n",
    "                .withColumn(\"phone_formatted\", format_phone_udf(col(\"phone\"))) \\\n",
    "                .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "                .withColumn(\"customer_segment\",\n",
    "                    when(col(\"lifetime_value\") > 5000, \"High Value\")\n",
    "                    .when(col(\"lifetime_value\") > 1000, \"Medium Value\")\n",
    "                    .otherwise(\"Low Value\")\n",
    "                ) \\\n",
    "                .filter(col(\"customer_id\").isNotNull())\n",
    "            \n",
    "            # Product transformations\n",
    "            products_clean = raw_data[\"products\"] \\\n",
    "                .withColumn(\"profit_margin\", \n",
    "                    round((col(\"price\") - col(\"cost\")) / col(\"price\") * 100, 2)) \\\n",
    "                .withColumn(\"price_category\",\n",
    "                    when(col(\"price\") > 100, \"Premium\")\n",
    "                    .when(col(\"price\") > 50, \"Standard\")\n",
    "                    .otherwise(\"Budget\")\n",
    "                ) \\\n",
    "                .filter(col(\"product_id\").isNotNull())\n",
    "            \n",
    "            # Transaction enrichment\n",
    "            transactions_enriched = raw_data[\"transactions\"] \\\n",
    "                .join(products_clean, \"product_id\", \"left\") \\\n",
    "                .join(customers_clean.select(\"customer_id\", \"customer_segment\", \"country\"), \n",
    "                      \"customer_id\", \"left\") \\\n",
    "                .withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
    "                .withColumn(\"profit\", col(\"quantity\") * (col(\"price\") - col(\"cost\"))) \\\n",
    "                .withColumn(\"transaction_hour\", hour(col(\"transaction_timestamp\"))) \\\n",
    "                .withColumn(\"transaction_day_of_week\", dayofweek(col(\"transaction_date\"))) \\\n",
    "                .filter(col(\"transaction_id\").isNotNull())\n",
    "            \n",
    "            # Validate transformations\n",
    "            self.validate_data(customers_clean, \"customers_transformed\", \n",
    "                             [\"customer_id\", \"customer_segment\"])\n",
    "            self.validate_data(transactions_enriched, \"transactions_transformed\", \n",
    "                             [\"transaction_id\", \"revenue\", \"profit\"])\n",
    "            \n",
    "            return {\n",
    "                \"dim_customers\": customers_clean,\n",
    "                \"dim_products\": products_clean,\n",
    "                \"fact_transactions\": transactions_enriched\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Transformation failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data(self, transformed_data):\n",
    "        \"\"\"Load data to Delta tables\"\"\"\n",
    "        logger.info(\"Starting data load...\")\n",
    "        \n",
    "        try:\n",
    "            # Load dimensions with SCD Type 1 (overwrite)\n",
    "            for table_name in [\"dim_customers\", \"dim_products\"]:\n",
    "                logger.info(f\"Loading {table_name}...\")\n",
    "                \n",
    "                transformed_data[table_name].write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                    .saveAsTable(f\"globalmart.{table_name}_pipeline\")\n",
    "                \n",
    "                count = self.spark.table(f\"globalmart.{table_name}_pipeline\").count()\n",
    "                self.log_metric(f\"{table_name}_loaded\", count)\n",
    "            \n",
    "            # Load fact table with append\n",
    "            logger.info(\"Loading fact_transactions...\")\n",
    "            \n",
    "            transformed_data[\"fact_transactions\"] \\\n",
    "                .withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "                .write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"transaction_date\") \\\n",
    "                .saveAsTable(\"globalmart.fact_transactions_pipeline\")\n",
    "            \n",
    "            count = transformed_data[\"fact_transactions\"].count()\n",
    "            self.log_metric(\"fact_transactions_loaded\", count)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Load failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute complete pipeline\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(\"Starting GlobalMart ETL Pipeline\")\n",
    "        logger.info(f\"Run timestamp: {datetime.now()}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract\n",
    "            raw_data = self.extract_data()\n",
    "            \n",
    "            # Transform\n",
    "            transformed_data = self.transform_data(raw_data)\n",
    "            \n",
    "            # Load\n",
    "            self.load_data(transformed_data)\n",
    "            \n",
    "            # Calculate total runtime\n",
    "            runtime = time.time() - self.start_time\n",
    "            self.log_metric(\"total_runtime_seconds\", runtime)\n",
    "            \n",
    "            # Save metrics\n",
    "            self.save_metrics()\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            logger.info(\"=\"*50)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            runtime = time.time() - self.start_time\n",
    "            self.log_metric(\"total_runtime_seconds\", runtime)\n",
    "            self.log_metric(\"status\", \"FAILED\")\n",
    "            self.log_metric(\"error\", str(e))\n",
    "            \n",
    "            logger.error(f\"Pipeline failed after {runtime:.2f} seconds\")\n",
    "            logger.error(f\"Error: {str(e)}\")\n",
    "            logger.info(\"=\"*50)\n",
    "            \n",
    "            # Save metrics even on failure\n",
    "            self.save_metrics()\n",
    "            \n",
    "            raise\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save pipeline metrics for monitoring\"\"\"\n",
    "        metrics_data = [{\n",
    "            \"run_timestamp\": datetime.now(),\n",
    "            \"metric_name\": k,\n",
    "            \"metric_value\": str(v)\n",
    "        } for k, v in self.metrics.items()]\n",
    "        \n",
    "        metrics_df = self.spark.createDataFrame(metrics_data)\n",
    "        \n",
    "        metrics_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(\"globalmart.pipeline_metrics\")\n",
    "\n",
    "# Create pipeline configuration\n",
    "pipeline_config = {\n",
    "    \"max_nulls_percentage\": 0.1,\n",
    "    \"batch_size\": 10000,\n",
    "    \"error_tolerance\": 0.05\n",
    "}\n",
    "\n",
    "print(\"✓ Pipeline framework created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90066a0-49a2-4bd8-8d29-d5ef45bde00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 6.2: Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef290afa-8c9f-4a88-b37a-5ef6c999073e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize and run pipeline\n",
    "pipeline = ETLPipeline(spark, pipeline_config)\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run()\n",
    "print(\"\\nPipeline Metrics:\")\n",
    "for metric, value in pipeline.metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08f7069-85b7-4513-a8ba-021de7721fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 6.3: Monitor Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e9fcd9-a0ee-4313-ac2f-7befc77ad4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check pipeline output\n",
    "print(\"Pipeline Output Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check loaded tables\n",
    "tables = [\n",
    "    \"globalmart.dim_customers_pipeline\",\n",
    "    \"globalmart.dim_products_pipeline\", \n",
    "    \"globalmart.fact_transactions_pipeline\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    count = spark.table(table).count()\n",
    "    print(f\"{table}: {count:,} records\")\n",
    "\n",
    "# View metrics history\n",
    "print(\"\\nPipeline Metrics History:\")\n",
    "metrics_history = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        run_timestamp,\n",
    "        metric_name,\n",
    "        metric_value\n",
    "    FROM globalmart.pipeline_metrics\n",
    "    ORDER BY run_timestamp DESC, metric_name\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(metrics_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f1494d-2df9-4aa5-ad90-efdbeb3ac4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 6.4: Create Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce6a115-f554-4dd3-9161-a7bc62f686ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create monitoring views\n",
    "# Data quality dashboard\n",
    "data_quality_dashboard = spark.sql(\"\"\"\n",
    "    WITH quality_metrics AS (\n",
    "        SELECT \n",
    "            'Customers' as dataset,\n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN email_valid = true THEN 1 ELSE 0 END) as valid_emails,\n",
    "            COUNT(DISTINCT customer_segment) as segments,\n",
    "            MIN(registration_date) as earliest_date,\n",
    "            MAX(registration_date) as latest_date\n",
    "        FROM globalmart.dim_customers_pipeline\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Products' as dataset,\n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT category) as valid_emails,  -- Using as category count\n",
    "            COUNT(DISTINCT price_category) as segments,\n",
    "            NULL as earliest_date,\n",
    "            NULL as latest_date\n",
    "        FROM globalmart.dim_products_pipeline\n",
    "    )\n",
    "    SELECT * FROM quality_metrics\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data Quality Dashboard:\")\n",
    "display(data_quality_dashboard)\n",
    "\n",
    "# Performance trends\n",
    "performance_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(run_timestamp) as run_date,\n",
    "        MAX(CASE WHEN metric_name = 'total_runtime_seconds' \n",
    "            THEN CAST(metric_value AS FLOAT) END) as runtime_seconds,\n",
    "        MAX(CASE WHEN metric_name = 'customers_count' \n",
    "            THEN CAST(metric_value AS INT) END) as customers_processed,\n",
    "        MAX(CASE WHEN metric_name = 'fact_transactions_loaded' \n",
    "            THEN CAST(metric_value AS INT) END) as transactions_loaded\n",
    "    FROM globalmart.pipeline_metrics\n",
    "    GROUP BY DATE(run_timestamp)\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPerformance Trends:\")\n",
    "display(performance_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18037fec-0d37-46a8-90e0-e008ea2a60ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 💡 Lab 6 Key Takeaways\n",
    "- Production pipelines need comprehensive error handling\n",
    "- Validate data at each stage of processing\n",
    "- Log metrics for monitoring and optimization\n",
    "- Use Delta tables for reliable data storage\n",
    "- Build reusable frameworks for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0399172-c14d-4c69-97ad-7b4e02080f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 🎉 Module 2 Complete!\n",
    "\n",
    "### Your Achievements\n",
    "You've successfully completed Module 2 and built production-grade ETL pipelines! Here's what you've mastered:\n",
    "\n",
    "1. **DataFrame Operations**: Complex transformations, joins, and aggregations\n",
    "2. **File Management**: Working with DBFS and multiple file formats\n",
    "3. **Spark SQL**: Advanced analytics with window functions\n",
    "4. **Delta Tables**: ACID transactions, merges, and time travel\n",
    "5. **UDFs**: Custom business logic with performance optimization\n",
    "6. **Complete Pipeline**: End-to-end ETL with monitoring and error handling\n",
    "\n",
    "### Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c886706-dc35-4e56-8bb3-c0d22b5c575d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate learning summary\n",
    "print(\"=\" * 60)\n",
    "print(\"MODULE 2 LEARNING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count what we've created\n",
    "table_count = spark.sql(\"SHOW TABLES IN globalmart\").count()\n",
    "print(f\"\\n📊 Tables created: {table_count}\")\n",
    "\n",
    "# Total records processed\n",
    "total_records = 0\n",
    "for table in spark.sql(\"SHOW TABLES IN globalmart\").collect():\n",
    "    if table.isTemporary == False:\n",
    "        count = spark.table(f\"globalmart.{table.tableName}\").count()\n",
    "        total_records += count\n",
    "\n",
    "print(f\"📈 Total records processed: {total_records:,}\")\n",
    "\n",
    "# Skills checklist\n",
    "skills = [\n",
    "    \"PySpark DataFrame transformations\",\n",
    "    \"File system operations with dbutils\",\n",
    "    \"Spark SQL and window functions\",\n",
    "    \"Delta Lake operations\",\n",
    "    \"UDF creation and optimization\",\n",
    "    \"Production pipeline development\",\n",
    "    \"Error handling and monitoring\",\n",
    "    \"Performance optimization\"\n",
    "]\n",
    "\n",
    "print(\"\\n✅ Skills Mastered:\")\n",
    "for skill in skills:\n",
    "    print(f\"   - {skill}\")\n",
    "\n",
    "print(\"\\n🚀 You're now ready for Module 3: Incremental Processing with Delta Lake!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ee5ebcb-2ce1-4202-80df-add5f74d6a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Before You Go\n",
    "\n",
    "**Remember to:**\n",
    "1. Save this notebook for future reference\n",
    "2. Review any sections that were challenging\n",
    "3. Practice the patterns with your own data\n",
    "4. Prepare questions for the Wednesday session\n",
    "\n",
    "**Optional Challenges:**\n",
    "- Optimize the pipeline to run 50% faster\n",
    "- Add data quality rules using Delta Live Tables expectations\n",
    "- Implement streaming ingestion for real-time data\n",
    "- Create a data lineage visualization\n",
    "\n",
    "Great work today! See you in Module 3! 🎓"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "module2-labs-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "name": "module2_etl_labs",
  "notebookId": "module2_etl_labs"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
