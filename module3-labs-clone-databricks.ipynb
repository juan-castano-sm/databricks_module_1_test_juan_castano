{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d886985-c50c-4a79-99a8-c5fb9ce5717f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 3: Incremental Data Processing and Delta Lake\n",
    "## Laboratory Exercises\n",
    "\n",
    "Welcome to the hands-on laboratory exercises for Module 3! Today we'll build sophisticated incremental processing systems using Auto Loader, Structured Streaming, and the Medallion Architecture.\n",
    "\n",
    "### Prerequisites\n",
    "- Access to your Databricks workspace\n",
    "- Running cluster (8.x or higher with Delta Lake)\n",
    "- Approximately 3.5 hours for completion\n",
    "\n",
    "### Lab Structure\n",
    "1. **Lab 1**: Auto Loader and Bronze Layer Implementation (45 minutes)\n",
    "2. **Lab 2**: Building Silver Layer with CDC (60 minutes)\n",
    "3. **Lab 3**: Structured Streaming and Gold Layer (60 minutes)\n",
    "4. **Lab 4**: Production Patterns and Monitoring (45 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "099662a6-a174-4266-af24-c9f88203c03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab Setup: Preparing Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334ca1ea-d3de-4a82-b77c-7fb9ba7848f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Module 3 Setup - Complete Environment Initialization\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Create module workspace\n",
    "module_path = \"dbfs:/databricks_course/module_03_incremental\"\n",
    "dbutils.fs.mkdirs(module_path)\n",
    "\n",
    "# Create dedicated database for Module 3\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS module3_incremental\")\n",
    "spark.sql(\"USE module3_incremental\")\n",
    "\n",
    "# Clean up any existing tables from previous runs\n",
    "tables_to_drop = [\n",
    "    \"bronze_sales\", \"silver_sales\", \"gold_realtime_sales_metrics\",\n",
    "    \"gold_sales_trends\", \"gold_customer_sessions\", \"gold_executive_dashboard\",\n",
    "    \"bronze_ingestion_metrics\", \"pipeline_health_metrics\"\n",
    "]\n",
    "\n",
    "for table in tables_to_drop:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "\n",
    "print(\"‚úÖ Environment initialized!\")\n",
    "print(f\"üìÅ Working directory: {module_path}\")\n",
    "print(f\"üóÑÔ∏è  Current database: {spark.catalog.currentDatabase()}\")\n",
    "print(f\"‚ö° Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183ee527-5401-439c-9c99-afaa6dbbd462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create data generators for realistic GlobalMart scenarios\n",
    "class GlobalMartDataGenerator:\n",
    "    \"\"\"Generates realistic streaming data for GlobalMart retail scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Product catalog with categories\n",
    "        self.products = [\n",
    "            {\"id\": f\"PROD_{i:04d}\", \n",
    "             \"name\": f\"Product {i}\", \n",
    "             \"category\": random.choice([\"Electronics\", \"Clothing\", \"Food\", \"Home\", \"Sports\"]),\n",
    "             \"price\": random.uniform(10, 500)}\n",
    "            for i in range(1, 101)\n",
    "        ]\n",
    "        \n",
    "        # Store locations across regions\n",
    "        self.stores = [\n",
    "            {\"id\": f\"STORE_{region}_{i:03d}\", \n",
    "             \"region\": region,\n",
    "             \"type\": random.choice([\"Flagship\", \"Standard\", \"Express\"])}\n",
    "            for region in [\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"]\n",
    "            for i in range(1, 6)\n",
    "        ]\n",
    "        \n",
    "        # Customer segments\n",
    "        self.customer_segments = [\"Premium\", \"Regular\", \"Budget\", \"New\"]\n",
    "        \n",
    "    def generate_sales_transaction(self, timestamp_base=None):\n",
    "        \"\"\"Generate a single sales transaction.\"\"\"\n",
    "        if timestamp_base is None:\n",
    "            timestamp_base = datetime.now()\n",
    "        \n",
    "        product = random.choice(self.products)\n",
    "        store = random.choice(self.stores)\n",
    "        \n",
    "        # Simulate late-arriving data (10% chance)\n",
    "        if random.random() < 0.1:\n",
    "            time_offset = -random.randint(0, 120)\n",
    "        else:\n",
    "            time_offset = 0\n",
    "        \n",
    "        transaction = {\n",
    "            \"transaction_id\": f\"TXN_{int(time.time() * 1000000)}_{random.randint(1000, 9999)}\",\n",
    "            \"store_id\": store[\"id\"],\n",
    "            \"store_region\": store[\"region\"],\n",
    "            \"product_id\": product[\"id\"],\n",
    "            \"product_category\": product[\"category\"],\n",
    "            \"customer_id\": f\"CUST_{random.randint(10000, 99999)}\",\n",
    "            \"customer_segment\": random.choice(self.customer_segments),\n",
    "            \"quantity\": random.randint(1, 10),\n",
    "            \"unit_price\": product[\"price\"],\n",
    "            \"transaction_time\": (timestamp_base + timedelta(minutes=time_offset)).isoformat(),\n",
    "            \"payment_method\": random.choice([\"credit_card\", \"debit_card\", \"cash\", \"mobile_pay\"]),\n",
    "            \"is_online\": random.choice([True, False]),\n",
    "            \"promotion_applied\": random.choice([True, False])\n",
    "        }\n",
    "        \n",
    "        # Calculate totals\n",
    "        base_amount = transaction[\"quantity\"] * transaction[\"unit_price\"]\n",
    "        if transaction[\"promotion_applied\"]:\n",
    "            discount = random.uniform(0.05, 0.25)\n",
    "            transaction[\"discount_amount\"] = base_amount * discount\n",
    "        else:\n",
    "            transaction[\"discount_amount\"] = 0.0\n",
    "        \n",
    "        transaction[\"total_amount\"] = base_amount - transaction[\"discount_amount\"]        \n",
    "        return transaction\n",
    "    \n",
    "    def generate_batch(self, num_records=1000, timestamp_base=None):\n",
    "        \"\"\"Generate a batch of transactions.\"\"\"\n",
    "        return [self.generate_sales_transaction(timestamp_base) \n",
    "                for _ in range(num_records)]\n",
    "\n",
    "# Initialize data generator\n",
    "data_generator = GlobalMartDataGenerator()\n",
    "print(\"‚úÖ GlobalMart data generator initialized\")\n",
    "print(f\"üì¶ Products: {len(data_generator.products)}\")\n",
    "print(f\"üè™ Stores: {len(data_generator.stores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5043a3-b377-4fb3-9a00-8b2e96caa743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate initial streaming data files\n",
    "def generate_streaming_files(base_path, num_files=5, records_per_file=1000):\n",
    "    \"\"\"Generate JSON files simulating streaming data arrival.\"\"\"\n",
    "    # Clean up existing files\n",
    "    #dbutils.fs.rm(base_path, recurse=True)\n",
    "    #dbutils.fs.mkdirs(base_path)\n",
    "    \n",
    "    files_created = []\n",
    "    base_time = datetime.now() - timedelta(hours=num_files)\n",
    "    \n",
    "    for file_num in range(num_files):\n",
    "        # Generate transactions for this time period\n",
    "        file_time = base_time + timedelta(hours=file_num)\n",
    "        transactions = data_generator.generate_batch(records_per_file, file_time)\n",
    "        \n",
    "        # Write to JSON file\n",
    "        file_path = f\"{base_path}/sales_{file_num:04d}_{int(file_time.timestamp())}.json\"\n",
    "        json_content = \"\\n\".join([json.dumps(t) for t in transactions])\n",
    "        dbutils.fs.put(file_path, json_content, overwrite=True)\n",
    "        \n",
    "        files_created.append(file_path)\n",
    "        time.sleep(0.5)  # Simulate time between file arrivals\n",
    "    \n",
    "    return files_created\n",
    "\n",
    "# Generate initial data\n",
    "streaming_input_path = f\"{module_path}/streaming_input\"\n",
    "initial_files = generate_streaming_files(streaming_input_path, num_files=5, records_per_file=1000)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(initial_files)} initial data files\")\n",
    "print(f\"üìÅ Files location: {streaming_input_path}\")\n",
    "print(f\"üìä Total records: {5 * 1000:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12fd34f-9fcc-4ba8-a61b-7c44c39d17e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 1: Auto Loader and Bronze Layer Implementation (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Configure Auto Loader for incremental file ingestion\n",
    "- Set up Bronze tables with proper metadata\n",
    "- Handle schema evolution and malformed files\n",
    "- Monitor ingestion progress and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb761e7a-7788-4d05-973f-323b0879a34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.1: Basic Auto Loader Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0b38e8-d1e4-41a3-a528-6caee07aa54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26313999-90ab-4c94-bde0-aa7fec21cbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/databricks_course/module_03_incremental/streaming_input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14a1476-b6ab-4095-abf7-df2ab49405af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/databricks_course/module_03_incremental/checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec87932-945b-46e9-b8c9-189d71e5d0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure paths for Bronze layer\n",
    "bronze_table_path = f\"{module_path}/tables/bronze_sales\"\n",
    "bronze_checkpoint = f\"{module_path}/checkpoints/bronze_sales\"\n",
    "bronze_schema_location = f\"{module_path}/schemas/bronze_sales\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(f\"{module_path}/checkpoints/bronze_sales\", recurse=True)\n",
    "    print(\"‚úÖ Cleared old checkpoint\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No old checkpoint to clear\")\n",
    "\n",
    "\n",
    "# Configure Auto Loader stream for Bronze ingestion\n",
    "bronze_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", bronze_schema_location)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 1)\n",
    "    .load(streaming_input_path)\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file_name\", col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"file_modification_time\", col(\"_metadata.file_modification_time\"))\n",
    "    .withColumn(\"bronze_processing_date\", current_date())\n",
    ")\n",
    "\n",
    "# Start the Bronze ingestion stream\n",
    "bronze_query = (\n",
    "    bronze_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", bronze_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .table(\"bronze_sales\")\n",
    ")\n",
    "\n",
    "print(\"üöÄ Auto Loader stream started for Bronze layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a80564-17d6-4daf-8140-9358546b26be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(f\"{module_path}/checkpoints/bronze_sales\", recurse=True)\n",
    "    print(\"‚úÖ Cleared old checkpoint\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No old checkpoint to clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a880b02-083f-47bc-b2e5-52ab4dd3af41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "dbfs:/databricks_course/module_03_incremental/streaming_input/sales_evolved_0000_1753829818.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40adacf5-c5de-432d-a3d4-1bb1c222e7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monitor streaming progress\n",
    "time.sleep(20)\n",
    "\n",
    "# Query the Bronze table\n",
    "bronze_count = spark.table(\"bronze_sales\").count()\n",
    "print(f\"\\nTotal records in Bronze layer: {bronze_count:,}\")\n",
    "\n",
    "# Display sample records\n",
    "display(\n",
    "    spark.table(\"bronze_sales\")\n",
    "    .select(\"transaction_id\", \"store_id\", \"total_amount\", \"transaction_time\", \"ingestion_timestamp\")\n",
    "    .orderBy(col(\"ingestion_timestamp\").desc())\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cdc0253-ee86-4cd2-8f94-58d2fefcff19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.2: Handling Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181882e6-c894-4cbd-9ef5-0d5cebe9a094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate data with evolved schema\n",
    "def generate_evolved_schema_data():\n",
    "    \"\"\"Generate data with new fields.\"\"\"\n",
    "    transactions = []\n",
    "    \n",
    "    for _ in range(500):\n",
    "        transaction = data_generator.generate_sales_transaction()\n",
    "        # Add new fields\n",
    "        transaction[\"loyalty_card_id\"] = f\"LOYAL_{random.randint(1000, 9999)}\" if random.random() > 0.3 else None\n",
    "        transaction[\"loyalty_points_earned\"] = random.randint(10, 500) if transaction[\"loyalty_card_id\"] else 0\n",
    "        transaction[\"delivery_method\"] = random.choice([\"in_store\", \"curbside\", \"home_delivery\"])\n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "# Generate files with evolved schema\n",
    "for i in range(2):\n",
    "    evolved_data = generate_evolved_schema_data()\n",
    "    file_path = f\"{streaming_input_path}/sales_evolved_{i:04d}_{int(time.time())}.json\"\n",
    "    json_content = \"\\n\".join([json.dumps(t) for t in evolved_data])\n",
    "    dbutils.fs.put(file_path, json_content, overwrite=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"‚úÖ Generated files with evolved schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bc5078-9a8b-4deb-bdac-58994d9f78fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wait and check schema evolution\n",
    "time.sleep(30)\n",
    "\n",
    "# Check schema\n",
    "evolved_df = spark.table(\"bronze_sales\")\n",
    "print(\"Schema after evolution:\")\n",
    "evolved_df.printSchema()\n",
    "\n",
    "# Query records with new fields\n",
    "display(\n",
    "    evolved_df\n",
    "    .filter(col(\"loyalty_card_id\").isNotNull())\n",
    "    .select(\"transaction_id\", \"loyalty_card_id\", \"loyalty_points_earned\", \"delivery_method\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1831436c-e192-43fa-8e21-ca7d44a5c7fa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"transaction_time\":302},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753833210377}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a118e9fc-94d2-4a94-8856-0b6dd1b358f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop Bronze stream\n",
    "bronze_query.stop()\n",
    "print(\"‚èπÔ∏è  Bronze stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac8d97c-0ac8-415a-bc0e-fee98d8030b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 2: Building Silver Layer with CDC (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Implement Change Data Feed on Bronze tables\n",
    "- Create Silver layer transformations using merge\n",
    "- Handle late-arriving data scenarios\n",
    "- Build quality metrics and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9b1fb2-343b-4708-979a-e817c287dcc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.1: Enable Change Data Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d78bb38-11ce-46fb-b341-2fc81a0eb036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Change Data Feed on Bronze table\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE bronze_sales \n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Change Data Feed enabled on bronze_sales table\")\n",
    "\n",
    "# Get current version for CDF tracking\n",
    "current_version = spark.sql(\"DESCRIBE HISTORY bronze_sales LIMIT 1\").select(\"version\").collect()[0][0]\n",
    "print(f\"üìå Current Bronze table version: {current_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055d0cf0-0085-4bd5-bd3c-910581a77aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Silver table with business rules\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver_sales (\n",
    "        transaction_id STRING NOT NULL,\n",
    "        store_id STRING NOT NULL,\n",
    "        store_region STRING,\n",
    "        product_id STRING NOT NULL,\n",
    "        product_category STRING,\n",
    "        customer_id STRING NOT NULL,\n",
    "        customer_segment STRING,\n",
    "        transaction_timestamp TIMESTAMP,\n",
    "        transaction_date DATE,\n",
    "        quantity INT,\n",
    "        unit_price DECIMAL(10,2),\n",
    "        discount_amount DECIMAL(10,2),\n",
    "        total_amount DECIMAL(10,2),\n",
    "        payment_method STRING,\n",
    "        channel STRING,\n",
    "        delivery_method STRING,\n",
    "        promotion_applied BOOLEAN,\n",
    "        loyalty_card_id STRING,\n",
    "        loyalty_points_earned INT,\n",
    "        is_valid BOOLEAN,\n",
    "        validation_errors ARRAY<STRING>,\n",
    "        data_quality_score DECIMAL(3,2),\n",
    "        silver_processing_timestamp TIMESTAMP,\n",
    "        bronze_file_name STRING,\n",
    "        bronze_ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (transaction_date)\n",
    "    TBLPROPERTIES (\n",
    "        delta.enableChangeDataFeed = true,\n",
    "        delta.autoOptimize.optimizeWrite = true,\n",
    "        delta.autoOptimize.autoCompact = true\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Silver table created with CDF and optimization enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc4a8af-6cad-4cb7-88c0-a124317632d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.2: Implement Silver Layer Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66979353-4f08-48b9-a504-fdf5045033ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data quality rules and transformations\n",
    "def apply_silver_transformations(bronze_df):\n",
    "    \"\"\"Apply business rules and data quality checks for Silver layer.\"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    transformed_df = (\n",
    "        bronze_df\n",
    "        # Parse timestamps\n",
    "        .withColumn(\"transaction_timestamp\", \n",
    "                   to_timestamp(col(\"transaction_time\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"))\n",
    "        .withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\")))\n",
    "        \n",
    "        # Standardize channel\n",
    "        .withColumn(\"channel\", \n",
    "                   when(col(\"is_online\") == True, \"ONLINE\").otherwise(\"IN_STORE\"))\n",
    "        \n",
    "        # Handle nulls\n",
    "        .withColumn(\"discount_amount\",\n",
    "                   when(col(\"discount_amount\").isNull(), 0.0).otherwise(col(\"discount_amount\")))\n",
    "        .withColumn(\"loyalty_points_earned\",\n",
    "                   when(col(\"loyalty_points_earned\").isNull(), 0).otherwise(col(\"loyalty_points_earned\")))\n",
    "        \n",
    "        # Initialize validation\n",
    "        .withColumn(\"validation_errors\", array().cast(\"array<string>\"))\n",
    "    )\n",
    "    \n",
    "    # Apply validation rules\n",
    "    validation_rules = [\n",
    "        (col(\"total_amount\") <= 0, \"Invalid total amount\"),\n",
    "        (col(\"quantity\") <= 0, \"Invalid quantity\"),\n",
    "        (~col(\"customer_id\").rlike(\"^CUST_[0-9]{5}$\"), \"Invalid customer ID format\"),\n",
    "        (col(\"transaction_timestamp\").isNull(), \"Invalid timestamp\")\n",
    "    ]\n",
    "    \n",
    "    for condition, error_msg in validation_rules:\n",
    "        transformed_df = transformed_df.withColumn(\n",
    "            \"validation_errors\",\n",
    "            when(condition, array_union(col(\"validation_errors\"), array(lit(error_msg))))\n",
    "            .otherwise(col(\"validation_errors\"))\n",
    "        )\n",
    "    \n",
    "    # Final calculations\n",
    "    final_df = (\n",
    "        transformed_df\n",
    "        .withColumn(\"is_valid\", size(col(\"validation_errors\")) == 0)\n",
    "        .withColumn(\"data_quality_score\",\n",
    "                   when(size(col(\"validation_errors\")) == 0, 1.0)\n",
    "                   .when(size(col(\"validation_errors\")) == 1, 0.7)\n",
    "                   .otherwise(0.3))\n",
    "        .withColumn(\"silver_processing_timestamp\", current_timestamp())\n",
    "        .withColumn(\"bronze_file_name\", col(\"source_file_name\"))\n",
    "        .withColumn(\"bronze_ingestion_timestamp\", col(\"ingestion_timestamp\"))\n",
    "    )\n",
    "    \n",
    "    return final_df.drop(\"source_file_name\", \"ingestion_timestamp\", \"file_modification_time\", \n",
    "                         \"bronze_processing_date\", \"_metadata\", \"is_online\", \"transaction_time\")\n",
    "\n",
    "# Test transformation\n",
    "bronze_sample = spark.table(\"bronze_sales\").limit(100)\n",
    "silver_sample = apply_silver_transformations(bronze_sample)\n",
    "\n",
    "print(\"‚úÖ Silver transformation function created\")\n",
    "display(\n",
    "    silver_sample\n",
    "    .groupBy(\"is_valid\")\n",
    "    .agg(count(\"*\").alias(\"count\"), avg(\"data_quality_score\").alias(\"avg_quality\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48992ae-0b01-49f9-a61f-9b3a52f582e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(f\"{module_path}/checkpoints/silver_cdc\", recurse=True)\n",
    "    print(\"‚úÖ Cleared old checkpoint\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No old checkpoint to clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efba3073-7583-4746-bfe8-09f4de81f841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Implement streaming CDC from Bronze to Silver\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def create_bronze_to_silver_stream():\n",
    "    \"\"\"Create streaming pipeline from Bronze to Silver using CDC.\"\"\"\n",
    "    \n",
    "    # Read CDC stream from Bronze\n",
    "    bronze_cdc_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .option(\"startingVersion\", current_version)\n",
    "        .table(\"bronze_sales\")\n",
    "        .filter(col(\"_change_type\").isin([\"insert\", \"update_postimage\"]))\n",
    "    )\n",
    "    \n",
    "    # Apply transformations\n",
    "    silver_stream = apply_silver_transformations(bronze_cdc_stream)\n",
    "    \n",
    "    # Define merge function\n",
    "    def merge_to_silver(batch_df, batch_id):\n",
    "        \"\"\"Merge batch into Silver table.\"\"\"\n",
    "\n",
    "        # Deduplicate within batch\n",
    "        window_spec = Window.partitionBy(\"transaction_id\").orderBy(col(\"silver_processing_timestamp\").desc())\n",
    "        \n",
    "        deduped_df = (\n",
    "            batch_df\n",
    "            .drop(\"_rescued_data\")\n",
    "            .drop(\"_change_type\")\n",
    "            .drop(\"_commit_version\")\n",
    "            .drop(\"_commit_timestamp\")\n",
    "            .withColumn(\"row_rank\", row_number().over(window_spec))\n",
    "            .filter(col(\"row_rank\") == 1)\n",
    "            .drop(\"row_rank\")\n",
    "        )\n",
    "        \n",
    "        # Merge into Silver\n",
    "        silver_table = DeltaTable.forName(spark, \"silver_sales\")\n",
    "        \n",
    "        silver_table.alias(\"target\").merge(\n",
    "            deduped_df.alias(\"source\"),\n",
    "            \"target.transaction_id = source.transaction_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition = \"source.silver_processing_timestamp > target.silver_processing_timestamp\",\n",
    "            set = {col: f\"source.{col}\" for col in deduped_df.columns}\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        print(f\"Batch {batch_id}: Processed {batch_df.count()} records\")\n",
    "    \n",
    "    # Create streaming query\n",
    "    query = (\n",
    "        silver_stream\n",
    "        .writeStream\n",
    "        .foreachBatch(merge_to_silver)\n",
    "        .outputMode(\"update\")\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "        .option(\"checkpointLocation\", f\"{module_path}/checkpoints/silver_cdc\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Start CDC stream\n",
    "silver_cdc_query = create_bronze_to_silver_stream()\n",
    "print(\"üöÄ Bronze ‚Üí Silver CDC stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daad9119-e90a-4dab-979f-9626e628d369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM silver_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680d0308-f531-4eb5-b17e-1a1dee3dcc46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate more data and check Silver\n",
    "generate_streaming_files(streaming_input_path, num_files=2, records_per_file=500)\n",
    "time.sleep(45)\n",
    "\n",
    "# Check Silver statistics\n",
    "silver_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN is_valid THEN 1 ELSE 0 END) as valid_records,\n",
    "        AVG(data_quality_score) as avg_quality_score\n",
    "    FROM silver_sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Silver Layer Statistics:\")\n",
    "display(silver_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9617967e-b7e2-4036-b1a4-59c898bccbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_streaming_files(streaming_input_path, num_files=4, records_per_file=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e47d6c-da52-4934-b478-f2da0beb9784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop Silver CDC stream\n",
    "silver_cdc_query.stop()\n",
    "print(\"‚èπÔ∏è  Silver CDC stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50bd44dc-e5db-4093-a44a-fcb6453a1f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 3: Structured Streaming and Gold Layer (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Develop streaming aggregations for real-time metrics\n",
    "- Implement windowing operations\n",
    "- Create Gold layer tables optimized for queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8272f876-fc89-4424-8212-5c5ea24c01a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.1: Real-Time Sales Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cc6789-7d6b-44d6-a0c6-ec1dafd06f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS gold_realtime_sales_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244c7ead-f47c-48af-80e7-8bf50ccaf1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Gold table for real-time metrics\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold_realtime_sales_metrics (\n",
    "        window_start TIMESTAMP,\n",
    "        window_end TIMESTAMP,\n",
    "        store_id STRING,\n",
    "        store_region STRING,\n",
    "        total_transactions BIGINT,\n",
    "        total_revenue DECIMAL(20,2),\n",
    "        avg_transaction_value DECIMAL(14,6),\n",
    "        unique_customers BIGINT,\n",
    "        online_revenue DECIMAL(22,2),\n",
    "        store_revenue DECIMAL(22,2),\n",
    "        last_updated TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Gold real-time metrics table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8261e3-8f94-405f-951f-6b92c0a04ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(f\"{module_path}/checkpoints/gold_realtime_metrics\", recurse=True)\n",
    "    print(\"‚úÖ Cleared old checkpoint\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No old checkpoint to clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c181bf5-cc3e-4e1c-82e0-49a20ae55886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming aggregation\n",
    "def create_realtime_sales_metrics():\n",
    "    \"\"\"Create streaming aggregations for real-time dashboard.\"\"\"\n",
    "    \n",
    "    # Read from Silver with CDC\n",
    "    silver_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .table(\"silver_sales\")\n",
    "        .filter((col(\"_change_type\").isin([\"insert\", \"update_postimage\"])) & \n",
    "                (col(\"is_valid\") == True))\n",
    "    )\n",
    "    \n",
    "    # Apply watermarking\n",
    "    watermarked_stream = silver_stream.withWatermark(\"transaction_timestamp\", \"30 minutes\")\n",
    "    \n",
    "    # Create windowed aggregations\n",
    "    windowed_metrics = (\n",
    "        watermarked_stream\n",
    "        .groupBy(\n",
    "            window(col(\"transaction_timestamp\"), \"5 minutes\"),\n",
    "            col(\"store_id\"),\n",
    "            col(\"store_region\")\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_transactions\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "            approx_count_distinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            sum(when(col(\"channel\") == \"ONLINE\", col(\"total_amount\")).otherwise(0)).alias(\"online_revenue\"),\n",
    "            sum(when(col(\"channel\") == \"IN_STORE\", col(\"total_amount\")).otherwise(0)).alias(\"store_revenue\")\n",
    "        )\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            \"store_id\",\n",
    "            \"store_region\",\n",
    "            \"total_transactions\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_transaction_value\",\n",
    "            \"unique_customers\",\n",
    "            \"online_revenue\",\n",
    "            \"store_revenue\"\n",
    "        )\n",
    "        .withColumn(\"last_updated\", current_timestamp())\n",
    "    )\n",
    "    \n",
    "    # Write to Gold table\n",
    "    query = (\n",
    "        windowed_metrics\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"complete\")\n",
    "        .option(\"checkpointLocation\", f\"{module_path}/checkpoints/gold_realtime_metrics\")\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "        .table(\"gold_realtime_sales_metrics\")\n",
    "    )\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Start real-time metrics\n",
    "realtime_query = create_realtime_sales_metrics()\n",
    "print(\"üöÄ Real-time sales metrics stream started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308b5147-1003-453d-b7cf-1ca67e62cc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate continuous data\n",
    "import threading\n",
    "\n",
    "def generate_continuous_data():\n",
    "    for i in range(3):\n",
    "        generate_streaming_files(streaming_input_path, num_files=1, records_per_file=200)\n",
    "        time.sleep(15)\n",
    "\n",
    "# Start data generation in background\n",
    "data_thread = threading.Thread(target=generate_continuous_data)\n",
    "data_thread.start()\n",
    "\n",
    "# Wait and query metrics\n",
    "time.sleep(60)\n",
    "\n",
    "# Query latest metrics\n",
    "latest_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        window_start,\n",
    "        window_end,\n",
    "        store_region,\n",
    "        SUM(total_transactions) as transactions,\n",
    "        SUM(total_revenue) as revenue\n",
    "    FROM gold_realtime_sales_metrics\n",
    "    WHERE window_end >= current_timestamp() - INTERVAL 15 MINUTES\n",
    "    GROUP BY window_start, window_end, store_region\n",
    "    ORDER BY window_end DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Latest Sales Metrics:\")\n",
    "display(latest_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ce8932-e041-4d57-a175-ba6fc21ad277",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"window_end\":232},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753911831679}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "        window_start,\n",
    "        window_end,\n",
    "        store_region,\n",
    "        SUM(total_transactions) as transactions,\n",
    "        SUM(total_revenue) as revenue\n",
    "    FROM gold_realtime_sales_metrics\n",
    "    --WHERE window_end >= current_timestamp() - INTERVAL 15 MINUTES\n",
    "    GROUP BY window_start, window_end, store_region\n",
    "    ORDER BY window_end DESC\n",
    "    LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c9e6d1-571c-401c-a113-869cbc019653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop streaming query\n",
    "realtime_query.stop()\n",
    "print(\"‚èπÔ∏è  Real-time metrics stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee0f274c-70fb-41fb-8c4e-61103985e9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 4: Production Patterns and Monitoring (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Implement circuit breakers and error handling\n",
    "- Set up comprehensive monitoring\n",
    "- Create data quality dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6747a43-30dd-4c01-b521-6afacf3ac59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.1: Implement Circuit Breaker Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f9a0d1-cd70-4cb8-b5cc-cb4fc28176d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create monitoring table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pipeline_health_metrics (\n",
    "        pipeline_name STRING,\n",
    "        check_timestamp TIMESTAMP,\n",
    "        metric_name STRING,\n",
    "        metric_value DOUBLE,\n",
    "        status STRING,\n",
    "        alert_triggered BOOLEAN\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Pipeline health metrics table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1894a375-03d1-44d6-9660-9157309a2ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple circuit breaker implementation\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, reset_timeout=300):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.reset_timeout = reset_timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.is_open = False\n",
    "    \n",
    "    def record_success(self):\n",
    "        self.failure_count = 0\n",
    "        self.is_open = False\n",
    "    \n",
    "    def record_failure(self):\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.is_open = True\n",
    "            print(f\"Circuit breaker OPEN - {self.failure_count} failures\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def should_attempt(self):\n",
    "        if not self.is_open:\n",
    "            return True\n",
    "        \n",
    "        if time.time() - self.last_failure_time > self.reset_timeout:\n",
    "            print(\"Circuit breaker attempting reset\")\n",
    "            self.is_open = False\n",
    "            self.failure_count = 0\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Test circuit breaker\n",
    "cb = CircuitBreaker(failure_threshold=3)\n",
    "print(\"‚úÖ Circuit breaker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67c8ac1-c0c2-42a9-82bd-539612e91975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.2: Data Quality Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96cefe2-1a5b-4597-8440-1948446c3ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create data quality dashboard\n",
    "def create_quality_dashboard():\n",
    "    \"\"\"Create comprehensive quality metrics.\"\"\"\n",
    "    \n",
    "    # Overall quality\n",
    "    overall_quality = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            'Overall' as metric_category,\n",
    "            COUNT(*) as total_records,\n",
    "            AVG(CASE WHEN is_valid THEN 1 ELSE 0 END) * 100 as validity_rate,\n",
    "            AVG(data_quality_score) * 100 as avg_quality_score\n",
    "        FROM silver_sales\n",
    "    \"\"\")\n",
    "    \n",
    "    # Error breakdown\n",
    "    error_breakdown = spark.sql(\"\"\"\n",
    "        WITH errors AS (\n",
    "            SELECT explode(validation_errors) as error_type\n",
    "            FROM silver_sales\n",
    "            WHERE size(validation_errors) > 0\n",
    "        )\n",
    "        SELECT \n",
    "            error_type,\n",
    "            COUNT(*) as error_count\n",
    "        FROM errors\n",
    "        GROUP BY error_type\n",
    "        ORDER BY error_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"üìä Data Quality Dashboard:\")\n",
    "    print(\"\\n1. Overall Quality:\")\n",
    "    display(overall_quality)\n",
    "    \n",
    "    print(\"\\n2. Error Breakdown:\")\n",
    "    display(error_breakdown)\n",
    "\n",
    "# Generate dashboard\n",
    "create_quality_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87cbd0b-40b6-4b1d-9199-61df27b2b319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Module 3 Complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Bronze Layer' as layer,\n",
    "        (SELECT COUNT(*) FROM bronze_sales) as record_count\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Silver Layer',\n",
    "        (SELECT COUNT(*) FROM silver_sales)\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Gold Layer',\n",
    "        (SELECT COUNT(*) FROM gold_realtime_sales_metrics)\n",
    "\"\"\")\n",
    "\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You've mastered:\")\n",
    "print(\"- Auto Loader for incremental ingestion\")\n",
    "print(\"- Change Data Feed for CDC\")\n",
    "print(\"- Structured Streaming for real-time analytics\")\n",
    "print(\"- Medallion Architecture implementation\")\n",
    "print(\"- Production monitoring patterns\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7721028773323572,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "module3-labs-clone-databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
