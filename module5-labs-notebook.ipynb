{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33da06de-5c88-493f-b264-e4908ce9447c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 5: Data Governance Implementation Labs\n",
    "## Unity Catalog and Enterprise Security\n",
    "\n",
    "Welcome to the Module 5 hands-on labs! These exercises will guide you through implementing a comprehensive data governance solution using Unity Catalog.\n",
    "\n",
    "**Total Duration**: 4.5 hours\n",
    "**Prerequisites**: \n",
    "- Completed Modules 1-4\n",
    "- Unity Catalog enabled workspace\n",
    "- Admin privileges\n",
    "\n",
    "## Lab Overview\n",
    "1. Unity Catalog Setup and Configuration (45 min)\n",
    "2. Fine-Grained Security Implementation (60 min)\n",
    "3. Privacy and Compliance Controls (60 min)\n",
    "4. Audit and Monitoring (45 min)\n",
    "5. Governance Automation (60 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8622e610-d396-4896-a037-826eca754e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Initial Setup\n",
    "Run this cell first to set up your environment variables and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141bee0e-c8a9-4d2a-aaf2-40b05a298b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Set your unique identifier (replace with your initials or ID)\n",
    "USER_ID = \"student\"  # CHANGE THIS!\n",
    "\n",
    "# Catalog names for this module\n",
    "MAIN_CATALOG = f\"sm-training\"\n",
    "DEV_CATALOG = f\"dev_{USER_ID}\"\n",
    "PROD_CATALOG = f\"prod_{USER_ID}\"\n",
    "\n",
    "# Helper function for logging\n",
    "def log_progress(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "# Helper function for validation\n",
    "def validate_step(condition, success_msg, failure_msg):\n",
    "    if condition:\n",
    "        print(f\"✅ {success_msg}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ {failure_msg}\")\n",
    "        return False\n",
    "\n",
    "log_progress(\"Environment initialized successfully\")\n",
    "print(f\"Your catalogs will be: {MAIN_CATALOG}, {DEV_CATALOG}, {PROD_CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e3e909-f493-4705-b66a-a61dbe652b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 1: Unity Catalog Setup and Configuration (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Create and configure catalogs\n",
    "- Set up schemas with appropriate permissions\n",
    "- Implement basic access controls\n",
    "- Configure external storage locations\n",
    "\n",
    "### Background\n",
    "GlobalMart needs a governance structure that supports development, staging, and production environments while maintaining security and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8364a28f-508d-439c-9e24-90afe1b548ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.1: Create Catalog Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c934c8d-cdda-4173-9696-b9793cf1b202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create main catalog for governance examples\n",
    "#spark.sql(f\"CREATE CATALOG IF NOT EXISTS {MAIN_CATALOG} COMMENT 'Main catalog for governance examples'\")\n",
    "spark.sql(f\"USE CATALOG {MAIN_CATALOG}\")\n",
    "\n",
    "# Verify catalog creation\n",
    "catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "catalog_names = [row.catalog for row in catalogs]\n",
    "\n",
    "validate_step(\n",
    "    MAIN_CATALOG in catalog_names,\n",
    "    f\"Catalog {MAIN_CATALOG} created successfully\",\n",
    "    f\"Failed to create catalog {MAIN_CATALOG}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20aab458-c3cf-4962-8b20-71952b380880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create schemas within the catalog\n",
    "schemas = [\n",
    "    (\"raw\", \"Raw data ingestion layer\"),\n",
    "    (\"bronze\", \"Bronze layer - raw data with metadata\"),\n",
    "    (\"silver\", \"Silver layer - cleaned and validated data\"),\n",
    "    (\"gold\", \"Gold layer - business-ready aggregates\"),\n",
    "    (\"governance\", \"Governance metadata and audit tables\"),\n",
    "    (\"sandbox\", \"User experimentation area\")\n",
    "]\n",
    "\n",
    "for schema_name, comment in schemas:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS {MAIN_CATALOG}.{schema_name}\n",
    "        COMMENT '{comment}'\n",
    "    \"\"\")\n",
    "    log_progress(f\"Created schema: {schema_name}\")\n",
    "\n",
    "# Display schema structure\n",
    "display(spark.sql(f\"SHOW SCHEMAS IN {MAIN_CATALOG}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92559c3f-ceae-472a-a1ab-7179e0df435c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.2: Set Up Role-Based Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e353be7e-870e-4cc8-8407-108fee1b8980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: In a real environment, you would use actual groups\n",
    "# For this lab, we'll demonstrate the GRANT syntax\n",
    "\n",
    "# Define access patterns\n",
    "access_patterns = f\"\"\"\n",
    "-- Data Engineers: Full access to all schemas\n",
    "-- GRANT ALL PRIVILEGES ON CATALOG {MAIN_CATALOG} TO `data-engineers`;\n",
    "\n",
    "-- Analysts: Read access to silver and gold, write to sandbox\n",
    "-- GRANT USE CATALOG ON CATALOG {MAIN_CATALOG} TO `analysts`;\n",
    "-- GRANT USE SCHEMA ON SCHEMA {MAIN_CATALOG}.silver TO `analysts`;\n",
    "-- GRANT SELECT ON SCHEMA {MAIN_CATALOG}.silver TO `analysts`;\n",
    "-- GRANT USE SCHEMA ON SCHEMA {MAIN_CATALOG}.gold TO `analysts`;\n",
    "-- GRANT SELECT ON SCHEMA {MAIN_CATALOG}.gold TO `analysts`;\n",
    "-- GRANT ALL PRIVILEGES ON SCHEMA {MAIN_CATALOG}.sandbox TO `analysts`;\n",
    "\n",
    "-- Business Users: Read access to gold only\n",
    "-- GRANT USE CATALOG ON CATALOG {MAIN_CATALOG} TO `business-users`;\n",
    "-- GRANT USE SCHEMA ON SCHEMA {MAIN_CATALOG}.gold TO `business-users`;\n",
    "-- GRANT SELECT ON SCHEMA {MAIN_CATALOG}.gold TO `business-users`;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Access control patterns (uncomment and modify for your environment):\")\n",
    "print(access_patterns)\n",
    "\n",
    "# Create a sample access control table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.access_matrix (\n",
    "        role STRING,\n",
    "        catalog STRING,\n",
    "        schema STRING,\n",
    "        table_pattern STRING,\n",
    "        permissions ARRAY<STRING>,\n",
    "        business_justification STRING,\n",
    "        last_reviewed DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Access control matrix table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8847215a-f577-41ae-9210-3c7b9a9ce841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1.3: Create Sample Tables for Governance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d933beea-03df-49cc-bbf9-7958c6adeb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample customer table with PII\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.silver.customers (\n",
    "        customer_id BIGINT,\n",
    "        first_name STRING COMMENT 'PII - Personal Name',\n",
    "        last_name STRING COMMENT 'PII - Personal Name',\n",
    "        email STRING COMMENT 'PII - Email Address',\n",
    "        phone STRING COMMENT 'PII - Phone Number',\n",
    "        date_of_birth DATE COMMENT 'PII - Sensitive',\n",
    "        ssn STRING COMMENT 'PII - Highly Sensitive',\n",
    "        address_line1 STRING COMMENT 'PII - Address',\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        postal_code STRING,\n",
    "        customer_since DATE,\n",
    "        lifetime_value DECIMAL(10,2),\n",
    "        preferred_contact_method STRING,\n",
    "        marketing_consent BOOLEAN,\n",
    "        last_updated TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Customer master data with PII - requires governance controls'\n",
    "\"\"\")\n",
    "\n",
    "# Create sample transactions table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.silver.transactions (\n",
    "        transaction_id STRING,\n",
    "        customer_id BIGINT,\n",
    "        transaction_date DATE,\n",
    "        amount DECIMAL(10,2),\n",
    "        currency STRING,\n",
    "        payment_method STRING,\n",
    "        merchant_name STRING,\n",
    "        merchant_category STRING,\n",
    "        transaction_status STRING,\n",
    "        region STRING COMMENT 'Used for row-level security'\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (transaction_date)\n",
    "    COMMENT 'Transaction data requiring row-level security by region'\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Sample tables created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "974ffb8e-7869-4f83-83e3-b687e37e7591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.silver.customers VALUES\n",
    "    (1001, 'John', 'Doe', 'john.doe@email.com', '555-0101', '1980-01-15', '123-45-6789', \n",
    "     '123 Main St', 'New York', 'NY', 'US', '10001', '2020-01-15', 5432.10, 'email', true, current_timestamp()),\n",
    "    (1002, 'Jane', 'Smith', 'jane.smith@email.com', '555-0102', '1985-05-20', '987-65-4321', \n",
    "     '456 Oak Ave', 'Los Angeles', 'CA', 'US', '90001', '2019-03-10', 8921.50, 'phone', true, current_timestamp()),\n",
    "    (1003, 'Robert', 'Johnson', 'r.johnson@email.com', '555-0103', '1978-11-30', '456-78-9012', \n",
    "     '789 Pine Rd', 'London', 'LDN', 'UK', 'SW1A 1AA', '2021-06-01', 3210.75, 'email', false, current_timestamp()),\n",
    "    (1004, 'Maria', 'Garcia', 'maria.g@email.com', '555-0104', '1990-03-25', '321-54-9876', \n",
    "     '321 Elm St', 'Madrid', 'MAD', 'ES', '28001', '2020-09-15', 6789.25, 'email', true, current_timestamp()),\n",
    "    (1005, 'Li', 'Wang', 'li.wang@email.com', '555-0105', '1982-07-12', '654-32-1098', \n",
    "     '654 Bamboo Ln', 'Singapore', 'SG', 'SG', '238839', '2018-11-20', 12543.80, 'phone', true, current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample transactions\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.silver.transactions VALUES\n",
    "    ('TXN001', 1001, '2024-01-15', 150.00, 'USD', 'credit_card', 'GlobalMart NYC', 'retail', 'completed', 'US'),\n",
    "    ('TXN002', 1002, '2024-01-15', 89.50, 'USD', 'debit_card', 'GlobalMart LA', 'retail', 'completed', 'US'),\n",
    "    ('TXN003', 1003, '2024-01-16', 220.00, 'GBP', 'credit_card', 'GlobalMart London', 'retail', 'completed', 'UK'),\n",
    "    ('TXN004', 1004, '2024-01-16', 175.50, 'EUR', 'paypal', 'GlobalMart Madrid', 'retail', 'completed', 'EU'),\n",
    "    ('TXN005', 1005, '2024-01-17', 320.00, 'SGD', 'credit_card', 'GlobalMart Singapore', 'retail', 'completed', 'APAC')\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Sample data inserted\")\n",
    "display(spark.sql(f\"SELECT COUNT(*) as customer_count FROM {MAIN_CATALOG}.silver.customers\"))\n",
    "display(spark.sql(f\"SELECT COUNT(*) as transaction_count FROM {MAIN_CATALOG}.silver.transactions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca50810f-795e-4e0c-b780-133ae3a0f7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 2: Fine-Grained Security Implementation (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Implement column-level security with data masking\n",
    "- Set up row-level security filters\n",
    "- Create tag-based access policies\n",
    "- Test security controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f59cbdd7-56c7-4d8c-a1b4-4250105959fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.1: Implement Column-Level Security with Data Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d14574-896c-4da1-b8ab-76182d5576ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create masking functions for different data types\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {MAIN_CATALOG}.governance.mask_email(email STRING)\n",
    "    RETURNS STRING\n",
    "    RETURN \n",
    "      CASE \n",
    "        WHEN is_member('data-engineers') THEN email\n",
    "        WHEN is_member('analysts') THEN concat(left(email, 3), '****@****')\n",
    "        ELSE 'REDACTED'\n",
    "      END\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {MAIN_CATALOG}.governance.mask_ssn(ssn STRING)\n",
    "    RETURNS STRING\n",
    "    RETURN \n",
    "      CASE \n",
    "        WHEN is_member('data-engineers') THEN ssn\n",
    "        WHEN is_member('hr-authorized') THEN concat('XXX-XX-', right(ssn, 4))\n",
    "        ELSE 'XXX-XX-XXXX'\n",
    "      END\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {MAIN_CATALOG}.governance.mask_phone(phone STRING)\n",
    "    RETURNS STRING  \n",
    "    RETURN\n",
    "      CASE\n",
    "        WHEN is_member('data-engineers') OR is_member('customer-service') THEN phone\n",
    "        ELSE concat(left(phone, 3), '-***-****')\n",
    "      END\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Masking functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea8ceb6f-532d-4f6b-a70c-2c325e4daf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply masking functions to columns\n",
    "# Note: In production, you would apply these masks. For demo, we'll show the syntax\n",
    "\n",
    "masking_commands = f\"\"\"\n",
    "-- Apply email masking\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN email SET MASK {MAIN_CATALOG}.governance.mask_email;\n",
    "\n",
    "-- Apply SSN masking\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN ssn SET MASK {MAIN_CATALOG}.governance.mask_ssn;\n",
    "\n",
    "-- Apply phone masking  \n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN phone SET MASK {MAIN_CATALOG}.governance.mask_phone;\n",
    "\"\"\"\n",
    "\n",
    "print(\"To apply masking in production, run:\")\n",
    "print(masking_commands)\n",
    "\n",
    "# Create a view demonstrating masked data\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.gold.customers_masked AS\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        {MAIN_CATALOG}.governance.mask_email(email) as email,\n",
    "        {MAIN_CATALOG}.governance.mask_phone(phone) as phone,\n",
    "        date_of_birth,\n",
    "        {MAIN_CATALOG}.governance.mask_ssn(ssn) as ssn,\n",
    "        city,\n",
    "        state,\n",
    "        country,\n",
    "        customer_since,\n",
    "        lifetime_value,\n",
    "        marketing_consent\n",
    "    FROM {MAIN_CATALOG}.silver.customers\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Masked view created\")\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.gold.customers_masked LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a71dc1-496b-4d62-909b-0852109b037d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.2: Implement Row-Level Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3814eab-c8e3-4177-be40-6b1dab96ba4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create row filter function based on region\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {MAIN_CATALOG}.governance.region_filter(region STRING)\n",
    "    RETURNS BOOLEAN\n",
    "    RETURN \n",
    "      CASE\n",
    "        WHEN is_member('global-admins') THEN TRUE\n",
    "        WHEN is_member('us-team') AND region = 'US' THEN TRUE\n",
    "        WHEN is_member('eu-team') AND region = 'EU' THEN TRUE\n",
    "        WHEN is_member('uk-team') AND region = 'UK' THEN TRUE\n",
    "        WHEN is_member('apac-team') AND region = 'APAC' THEN TRUE\n",
    "        ELSE FALSE\n",
    "      END\n",
    "\"\"\")\n",
    "\n",
    "# Create a mapping table for row-level security\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.user_region_access (\n",
    "        user_email STRING,\n",
    "        allowed_regions ARRAY<STRING>,\n",
    "        access_level STRING,\n",
    "        valid_from DATE,\n",
    "        valid_until DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample access mappings\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.user_region_access VALUES\n",
    "    ('admin@globalmart.com', array('US', 'EU', 'UK', 'APAC'), 'FULL', '2024-01-01', '2025-12-31'),\n",
    "    ('us.manager@globalmart.com', array('US'), 'READ', '2024-01-01', '2025-12-31'),\n",
    "    ('eu.analyst@globalmart.com', array('EU', 'UK'), 'READ', '2024-01-01', '2025-12-31'),\n",
    "    ('apac.lead@globalmart.com', array('APAC'), 'READ', '2024-01-01', '2025-12-31')\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Row-level security functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21afd729-9b97-4d87-b977-e1ceb763bddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create filtered views demonstrating row-level security\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.gold.transactions_regional AS\n",
    "    SELECT t.*\n",
    "    FROM {MAIN_CATALOG}.silver.transactions t\n",
    "    WHERE {MAIN_CATALOG}.governance.region_filter(t.region)\n",
    "\"\"\")\n",
    "\n",
    "# Create a view showing what each user can access\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.effective_permissions AS\n",
    "    WITH user_context AS (\n",
    "        SELECT \n",
    "            current_user() as current_user,\n",
    "            current_date() as check_date\n",
    "    )\n",
    "    SELECT \n",
    "        u.current_user,\n",
    "        ura.allowed_regions,\n",
    "        ura.access_level,\n",
    "        COUNT(DISTINCT t.transaction_id) as accessible_transactions\n",
    "    FROM user_context u\n",
    "    LEFT JOIN {MAIN_CATALOG}.governance.user_region_access ura\n",
    "        ON u.current_user = ura.user_email\n",
    "        AND u.check_date BETWEEN ura.valid_from AND ura.valid_until\n",
    "    LEFT JOIN {MAIN_CATALOG}.silver.transactions t\n",
    "        ON array_contains(ura.allowed_regions, t.region)\n",
    "    GROUP BY u.current_user, ura.allowed_regions, ura.access_level\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Regional access views created\")\n",
    "display(spark.sql(f\"SELECT region, COUNT(*) as transaction_count FROM {MAIN_CATALOG}.silver.transactions GROUP BY region\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94508f16-1722-49bd-bb8b-fb5f46df18e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2.3: Implement Tag-Based Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5136a3b5-568f-4e70-bcd0-7f970b187e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply tags to classify data sensitivity\n",
    "tagging_commands = f\"\"\"\n",
    "-- Tag entire table\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers \n",
    "SET TAGS ('contains_pii' = 'true', 'data_classification' = 'confidential');\n",
    "\n",
    "-- Tag specific columns with PII classification\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN email SET TAGS ('pii_type' = 'email', 'sensitivity' = 'medium');\n",
    "\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN ssn SET TAGS ('pii_type' = 'national_id', 'sensitivity' = 'high', 'regulated' = 'true');\n",
    "\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.customers\n",
    "ALTER COLUMN date_of_birth SET TAGS ('pii_type' = 'birthdate', 'sensitivity' = 'medium');\n",
    "\n",
    "-- Tag transaction table\n",
    "ALTER TABLE {MAIN_CATALOG}.silver.transactions\n",
    "SET TAGS ('contains_pii' = 'false', 'data_classification' = 'internal');\n",
    "\"\"\"\n",
    "\n",
    "# Execute tagging (in production Unity Catalog)\n",
    "for command in tagging_commands.strip().split(';'):\n",
    "    if command.strip():\n",
    "        try:\n",
    "            spark.sql(command)\n",
    "            print(f\"✅ Executed: {command.strip()[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"ℹ️ Tag command (would run in production): {command.strip()[:50]}...\")\n",
    "\n",
    "# Create a tag inventory table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.data_classification_tags (\n",
    "        tag_name STRING,\n",
    "        tag_value STRING,\n",
    "        description STRING,\n",
    "        compliance_requirement STRING,\n",
    "        handling_instructions STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.data_classification_tags VALUES\n",
    "    ('sensitivity', 'high', 'Highly sensitive data requiring maximum protection', 'GDPR, CCPA', 'Encrypt at rest and in transit, audit all access'),\n",
    "    ('sensitivity', 'medium', 'Moderately sensitive data', 'GDPR, CCPA', 'Encrypt at rest, mask in non-prod'),\n",
    "    ('sensitivity', 'low', 'Low sensitivity data', 'None', 'Standard controls'),\n",
    "    ('pii_type', 'national_id', 'National identification numbers (SSN, etc)', 'GDPR Article 9', 'Never display in logs, maximum encryption'),\n",
    "    ('pii_type', 'email', 'Email addresses', 'GDPR, CAN-SPAM', 'Hash for analytics, mask in non-prod'),\n",
    "    ('regulated', 'true', 'Subject to regulatory compliance', 'Various', 'Maintain audit trail, restricted access')\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Tag-based classification implemented\")\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.governance.data_classification_tags\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73500b99-b375-4b9d-b691-7839b317aa5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 3: Privacy and Compliance Controls (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Implement GDPR compliance features (right to be forgotten)\n",
    "- Create data retention policies\n",
    "- Build consent management system\n",
    "- Implement cross-border data transfer controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b6581e-c918-4e9c-84f9-eaa3f84299dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.1: Implement Right to be Forgotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86db184-38c8-4a84-8802-e85a28ba31d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create deletion request tracking table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.deletion_requests (\n",
    "        request_id STRING,\n",
    "        customer_id BIGINT,\n",
    "        request_date TIMESTAMP,\n",
    "        requestor_email STRING,\n",
    "        reason STRING,\n",
    "        status STRING,\n",
    "        completed_date TIMESTAMP,\n",
    "        tables_affected ARRAY<STRING>,\n",
    "        records_deleted INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create a function to handle GDPR deletion\n",
    "def process_deletion_request(customer_id, reason=\"GDPR Request\"):\n",
    "    \"\"\"\n",
    "    Process a customer deletion request in compliance with GDPR\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    request_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Log the request\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.deletion_requests \n",
    "        (request_id, customer_id, request_date, requestor_email, reason, status)\n",
    "        VALUES ('{request_id}', {customer_id}, current_timestamp(), current_user(), '{reason}', 'PENDING')\n",
    "    \"\"\")\n",
    "    \n",
    "    affected_tables = []\n",
    "    total_deleted = 0\n",
    "    \n",
    "    try:\n",
    "        # Delete from customers table\n",
    "        result = spark.sql(f\"\"\"\n",
    "            DELETE FROM {MAIN_CATALOG}.silver.customers \n",
    "            WHERE customer_id = {customer_id}\n",
    "        \"\"\")\n",
    "        affected_tables.append(f\"{MAIN_CATALOG}.silver.customers\")\n",
    "        \n",
    "        # Delete from transactions table\n",
    "        result = spark.sql(f\"\"\"\n",
    "            DELETE FROM {MAIN_CATALOG}.silver.transactions \n",
    "            WHERE customer_id = {customer_id}\n",
    "        \"\"\")\n",
    "        affected_tables.append(f\"{MAIN_CATALOG}.silver.transactions\")\n",
    "        \n",
    "        # Update request status\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {MAIN_CATALOG}.governance.deletion_requests\n",
    "            SET status = 'COMPLETED',\n",
    "                completed_date = current_timestamp(),\n",
    "                tables_affected = array({','.join([f\"'{t}'\" for t in affected_tables])})\n",
    "            WHERE request_id = '{request_id}'\n",
    "        \"\"\")\n",
    "        \n",
    "        log_progress(f\"Successfully processed deletion request {request_id} for customer {customer_id}\")\n",
    "        return request_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Update request status to failed\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {MAIN_CATALOG}.governance.deletion_requests\n",
    "            SET status = 'FAILED'\n",
    "            WHERE request_id = '{request_id}'\n",
    "        \"\"\")\n",
    "        raise e\n",
    "\n",
    "# Test deletion request (don't actually run this on important data!)\n",
    "# request_id = process_deletion_request(1001, \"Test GDPR deletion\")\n",
    "print(\"✅ Deletion request function created\")\n",
    "print(\"⚠️ WARNING: Do not run the deletion function on important data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e8f86d4-d14f-4025-8cd9-8bccf885e7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.2: Implement Data Retention Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81137168-c3d6-4d09-8ab4-d82429c86fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create retention policy table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.retention_policies (\n",
    "        policy_id STRING,\n",
    "        table_name STRING,\n",
    "        retention_days INT,\n",
    "        deletion_column STRING,\n",
    "        policy_type STRING COMMENT 'HARD_DELETE or ANONYMIZE',\n",
    "        business_justification STRING,\n",
    "        compliance_requirement STRING,\n",
    "        is_active BOOLEAN,\n",
    "        last_run_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert retention policies\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.retention_policies VALUES\n",
    "    ('POL001', '{MAIN_CATALOG}.silver.customers', 2555, 'last_updated', 'ANONYMIZE', \n",
    "     'GDPR requires deletion after 7 years of inactivity', 'GDPR Article 5(e)', true, null),\n",
    "    ('POL002', '{MAIN_CATALOG}.silver.transactions', 2190, 'transaction_date', 'HARD_DELETE',\n",
    "     'Financial records retention for 6 years', 'SOX Compliance', true, null),\n",
    "    ('POL003', '{MAIN_CATALOG}.governance.audit_logs', 2920, 'event_date', 'HARD_DELETE',\n",
    "     'Audit logs retained for 8 years', 'Internal Policy', true, null)\n",
    "\"\"\")\n",
    "\n",
    "# Create procedure to enforce retention\n",
    "def enforce_retention_policy(policy_id):\n",
    "    \"\"\"\n",
    "    Enforce a specific retention policy\n",
    "    \"\"\"\n",
    "    policy = spark.sql(f\"\"\"\n",
    "        SELECT * FROM {MAIN_CATALOG}.governance.retention_policies\n",
    "        WHERE policy_id = '{policy_id}' AND is_active = true\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if not policy:\n",
    "        raise ValueError(f\"Policy {policy_id} not found or inactive\")\n",
    "    \n",
    "    p = policy[0]\n",
    "    cutoff_date = f\"current_date() - {p.retention_days}\"\n",
    "    \n",
    "    if p.policy_type == 'HARD_DELETE':\n",
    "        # Count before deletion\n",
    "        count_query = f\"\"\"\n",
    "            SELECT COUNT(*) as cnt FROM {p.table_name}\n",
    "            WHERE {p.deletion_column} < {cutoff_date}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Delete query\n",
    "        delete_query = f\"\"\"\n",
    "            DELETE FROM {p.table_name}\n",
    "            WHERE {p.deletion_column} < {cutoff_date}\n",
    "        \"\"\"\n",
    "        \n",
    "    elif p.policy_type == 'ANONYMIZE':\n",
    "        # Anonymization query\n",
    "        delete_query = f\"\"\"\n",
    "            UPDATE {p.table_name}\n",
    "            SET first_name = 'ANONYMIZED',\n",
    "                last_name = 'ANONYMIZED', \n",
    "                email = 'anonymized@anonymized.com',\n",
    "                phone = '000-000-0000',\n",
    "                ssn = '000-00-0000'\n",
    "            WHERE {p.deletion_column} < {cutoff_date}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Update last run date\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {MAIN_CATALOG}.governance.retention_policies\n",
    "        SET last_run_date = current_date()\n",
    "        WHERE policy_id = '{policy_id}'\n",
    "    \"\"\")\n",
    "    \n",
    "    return f\"Policy {policy_id} enforced successfully\"\n",
    "\n",
    "log_progress(\"Retention policy system created\")\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.governance.retention_policies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb1e84b-35d5-4e29-9225-0f720c568162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3.3: Implement Consent Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb9557e-bd2c-4658-a22d-55fbda0554fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create consent tracking table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.consent_records (\n",
    "        consent_id STRING,\n",
    "        customer_id BIGINT,\n",
    "        consent_type STRING,\n",
    "        consent_given BOOLEAN,\n",
    "        consent_date TIMESTAMP,\n",
    "        expiry_date DATE,\n",
    "        collection_method STRING,\n",
    "        ip_address STRING,\n",
    "        withdrawn_date TIMESTAMP\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample consent records\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.consent_records VALUES\n",
    "    ('CON001', 1001, 'marketing_emails', true, '2024-01-15 10:00:00', '2025-01-15', 'website_form', '192.168.1.1', null),\n",
    "    ('CON002', 1001, 'data_analytics', true, '2024-01-15 10:00:00', '2025-01-15', 'website_form', '192.168.1.1', null),\n",
    "    ('CON003', 1002, 'marketing_emails', true, '2024-01-10 14:30:00', '2025-01-10', 'mobile_app', '192.168.1.2', null),\n",
    "    ('CON004', 1002, 'data_analytics', false, '2024-01-10 14:30:00', null, 'mobile_app', '192.168.1.2', null),\n",
    "    ('CON005', 1003, 'marketing_emails', false, '2024-01-16 09:15:00', null, 'email_link', '192.168.1.3', null)\n",
    "\"\"\")\n",
    "\n",
    "# Create view for current consent status\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.gold.customer_consent_status AS\n",
    "    WITH latest_consent AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            consent_type,\n",
    "            consent_given,\n",
    "            consent_date,\n",
    "            expiry_date,\n",
    "            ROW_NUMBER() OVER (PARTITION BY customer_id, consent_type ORDER BY consent_date DESC) as rn\n",
    "        FROM {MAIN_CATALOG}.governance.consent_records\n",
    "    )\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        MAX(CASE WHEN lc.consent_type = 'marketing_emails' THEN lc.consent_given ELSE false END) as marketing_consent,\n",
    "        MAX(CASE WHEN lc.consent_type = 'data_analytics' THEN lc.consent_given ELSE false END) as analytics_consent,\n",
    "        MAX(CASE WHEN lc.consent_type = 'third_party_sharing' THEN lc.consent_given ELSE false END) as third_party_consent\n",
    "    FROM {MAIN_CATALOG}.silver.customers c\n",
    "    LEFT JOIN latest_consent lc ON c.customer_id = lc.customer_id AND lc.rn = 1\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Consent management system created\")\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.gold.customer_consent_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1dbbe4-1781-4b15-88bd-dce21f64d4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 4: Audit and Monitoring (45 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Create comprehensive audit logging system\n",
    "- Build data lineage tracking\n",
    "- Implement compliance dashboards\n",
    "- Set up alerting for policy violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb37270-9385-487c-98fe-297fb8360e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.1: Create Audit Logging System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f7dabfa-00c3-436c-a372-cd29454a47ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create custom audit log table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.custom_audit_logs (\n",
    "        event_id STRING,\n",
    "        event_time TIMESTAMP,\n",
    "        user_name STRING,\n",
    "        user_email STRING,\n",
    "        action STRING,\n",
    "        object_type STRING,\n",
    "        object_name STRING,\n",
    "        object_id STRING,\n",
    "        result STRING,\n",
    "        error_message STRING,\n",
    "        client_ip STRING,\n",
    "        session_id STRING,\n",
    "        additional_info MAP<STRING, STRING>\n",
    "    )\n",
    "    PARTITIONED BY (event_date DATE)\n",
    "\"\"\")\n",
    "\n",
    "# Function to log audit events\n",
    "def log_audit_event(action, object_type, object_name, object_id=None, result=\"SUCCESS\", error_message=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Log custom audit event\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "    \n",
    "    event_id = str(uuid.uuid4())\n",
    "    event_time = datetime.now()\n",
    "    \n",
    "    # Create additional info map\n",
    "    additional_info = {k: str(v) for k, v in kwargs.items()}\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.custom_audit_logs\n",
    "        VALUES (\n",
    "            '{event_id}',\n",
    "            '{event_time}',\n",
    "            current_user(),\n",
    "            current_user(),\n",
    "            '{action}',\n",
    "            '{object_type}',\n",
    "            '{object_name}',\n",
    "            {'NULL' if object_id is None else f\"'{object_id}'\"},\n",
    "            '{result}',\n",
    "            {'NULL' if error_message is None else f\"'{error_message}'\"},\n",
    "            '127.0.0.1',\n",
    "            '{spark.sparkContext.applicationId}',\n",
    "            map({','.join([f\"'{k}', '{v}'\" for k, v in additional_info.items()])}),\n",
    "            date('{event_time}')\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    return event_id\n",
    "\n",
    "# Test audit logging\n",
    "test_event_id = log_audit_event(\n",
    "    action=\"READ\",\n",
    "    object_type=\"TABLE\",\n",
    "    object_name=f\"{MAIN_CATALOG}.silver.customers\",\n",
    "    rows_accessed=\"5\",\n",
    "    query_type=\"SELECT\"\n",
    ")\n",
    "\n",
    "log_progress(f\"Audit logging system created. Test event ID: {test_event_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9b3dfb-8ab1-4d92-be61-3ea12abd8118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create audit analysis views\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.audit_summary AS\n",
    "    SELECT \n",
    "        event_date,\n",
    "        user_name,\n",
    "        action,\n",
    "        object_type,\n",
    "        COUNT(*) as event_count,\n",
    "        SUM(CASE WHEN result = 'SUCCESS' THEN 1 ELSE 0 END) as success_count,\n",
    "        SUM(CASE WHEN result = 'FAILURE' THEN 1 ELSE 0 END) as failure_count\n",
    "    FROM {MAIN_CATALOG}.governance.custom_audit_logs\n",
    "    GROUP BY event_date, user_name, action, object_type\n",
    "\"\"\")\n",
    "\n",
    "# Create suspicious activity detector\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.suspicious_activity AS\n",
    "    SELECT \n",
    "        user_name,\n",
    "        event_date,\n",
    "        COUNT(DISTINCT object_name) as unique_objects_accessed,\n",
    "        COUNT(*) as total_accesses,\n",
    "        COUNT(DISTINCT client_ip) as unique_ips,\n",
    "        COLLECT_SET(object_name) as accessed_objects\n",
    "    FROM {MAIN_CATALOG}.governance.custom_audit_logs\n",
    "    WHERE action IN ('READ', 'DOWNLOAD', 'EXPORT')\n",
    "        AND event_date >= current_date() - 7\n",
    "    GROUP BY user_name, event_date\n",
    "    HAVING COUNT(*) > 100 OR COUNT(DISTINCT object_name) > 20\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Audit analysis views created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e970f7c4-32ef-4dbd-a4dc-785c428c69e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.2: Implement Data Lineage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4033ed-d87e-42cb-850a-d8cc2cc8e017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create lineage tracking table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.data_lineage (\n",
    "        lineage_id STRING,\n",
    "        source_table STRING,\n",
    "        target_table STRING,\n",
    "        transformation_type STRING,\n",
    "        transformation_logic STRING,\n",
    "        created_by STRING,\n",
    "        created_date TIMESTAMP,\n",
    "        job_id STRING,\n",
    "        is_active BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create sample lineage records\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.data_lineage VALUES\n",
    "    ('LIN001', '{MAIN_CATALOG}.bronze.raw_customers', '{MAIN_CATALOG}.silver.customers', \n",
    "     'CLEANSE', 'Remove duplicates, validate emails, standardize formats', \n",
    "     'etl_job_user', '2024-01-01 00:00:00', 'JOB_001', true),\n",
    "    \n",
    "    ('LIN002', '{MAIN_CATALOG}.bronze.raw_transactions', '{MAIN_CATALOG}.silver.transactions',\n",
    "     'ENRICH', 'Join with customer data, calculate derived fields',\n",
    "     'etl_job_user', '2024-01-01 00:00:00', 'JOB_002', true),\n",
    "    \n",
    "    ('LIN003', '{MAIN_CATALOG}.silver.customers', '{MAIN_CATALOG}.gold.customer_360',\n",
    "     'AGGREGATE', 'Create customer 360 view with lifetime metrics',\n",
    "     'analytics_user', '2024-01-05 00:00:00', 'JOB_003', true),\n",
    "    \n",
    "    ('LIN004', '{MAIN_CATALOG}.silver.transactions', '{MAIN_CATALOG}.gold.sales_summary',\n",
    "     'AGGREGATE', 'Daily sales rollup by region and product',\n",
    "     'analytics_user', '2024-01-05 00:00:00', 'JOB_004', true)\n",
    "\"\"\")\n",
    "\n",
    "# Create lineage visualization view\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.lineage_graph AS\n",
    "    WITH RECURSIVE lineage_tree AS (\n",
    "        -- Base case: tables with no upstream dependencies\n",
    "        SELECT \n",
    "            target_table as table_name,\n",
    "            source_table,\n",
    "            target_table,\n",
    "            transformation_type,\n",
    "            1 as level,\n",
    "            CAST(target_table AS STRING) as path\n",
    "        FROM {MAIN_CATALOG}.governance.data_lineage\n",
    "        WHERE source_table LIKE '%bronze%'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Recursive case\n",
    "        SELECT \n",
    "            dl.target_table as table_name,\n",
    "            dl.source_table,\n",
    "            dl.target_table,\n",
    "            dl.transformation_type,\n",
    "            lt.level + 1 as level,\n",
    "            CONCAT(lt.path, ' -> ', dl.target_table) as path\n",
    "        FROM {MAIN_CATALOG}.governance.data_lineage dl\n",
    "        JOIN lineage_tree lt ON dl.source_table = lt.target_table\n",
    "        WHERE lt.level < 5  -- Prevent infinite recursion\n",
    "    )\n",
    "    SELECT DISTINCT * FROM lineage_tree\n",
    "    ORDER BY level, table_name\n",
    "\"\"\")\n",
    "\n",
    "log_progress(\"Data lineage tracking implemented\")\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.governance.data_lineage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca64e487-371d-4e73-a903-ff8921420808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 4.3: Create Compliance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4ce88d-b6c4-471e-b3a0-7f9159c4ef1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create compliance metrics table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.compliance_metrics (\n",
    "        metric_date DATE,\n",
    "        metric_name STRING,\n",
    "        metric_value DOUBLE,\n",
    "        metric_status STRING,\n",
    "        threshold_value DOUBLE,\n",
    "        measurement_details STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Function to calculate compliance metrics\n",
    "def calculate_compliance_metrics():\n",
    "    \"\"\"\n",
    "    Calculate daily compliance metrics\n",
    "    \"\"\"\n",
    "    metrics_date = datetime.now().date()\n",
    "    \n",
    "    # Metric 1: Tables with proper classification\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.compliance_metrics\n",
    "        SELECT \n",
    "            current_date() as metric_date,\n",
    "            'tables_classified_percentage' as metric_name,\n",
    "            100.0 as metric_value,  -- In real scenario, calculate from information_schema\n",
    "            CASE WHEN 100.0 >= 95 THEN 'PASS' ELSE 'FAIL' END as metric_status,\n",
    "            95.0 as threshold_value,\n",
    "            'Percentage of tables with data classification tags' as measurement_details\n",
    "    \"\"\")\n",
    "    \n",
    "    # Metric 2: Active consent percentage\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.compliance_metrics\n",
    "        WITH consent_stats AS (\n",
    "            SELECT \n",
    "                COUNT(DISTINCT customer_id) as total_customers,\n",
    "                SUM(CASE WHEN marketing_consent = true THEN 1 ELSE 0 END) as consented_customers\n",
    "            FROM {MAIN_CATALOG}.gold.customer_consent_status\n",
    "        )\n",
    "        SELECT \n",
    "            current_date() as metric_date,\n",
    "            'active_consent_percentage' as metric_name,\n",
    "            (consented_customers * 100.0 / total_customers) as metric_value,\n",
    "            'INFO' as metric_status,\n",
    "            0.0 as threshold_value,\n",
    "            'Percentage of customers with active marketing consent' as measurement_details\n",
    "        FROM consent_stats\n",
    "    \"\"\")\n",
    "    \n",
    "    # Metric 3: Data retention compliance\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.compliance_metrics\n",
    "        SELECT \n",
    "            current_date() as metric_date,\n",
    "            'retention_policies_current' as metric_name,\n",
    "            100.0 as metric_value,  -- Simplified for demo\n",
    "            CASE WHEN 100.0 = 100 THEN 'PASS' ELSE 'FAIL' END as metric_status,\n",
    "            100.0 as threshold_value,\n",
    "            'Percentage of retention policies run within SLA' as measurement_details\n",
    "    \"\"\")\n",
    "    \n",
    "    log_progress(\"Compliance metrics calculated\")\n",
    "\n",
    "# Calculate metrics\n",
    "calculate_compliance_metrics()\n",
    "\n",
    "# Create compliance dashboard view\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.compliance_dashboard AS\n",
    "    SELECT \n",
    "        metric_date,\n",
    "        metric_name,\n",
    "        metric_value,\n",
    "        threshold_value,\n",
    "        metric_status,\n",
    "        CASE \n",
    "            WHEN metric_status = 'PASS' THEN '✅'\n",
    "            WHEN metric_status = 'FAIL' THEN '❌'\n",
    "            ELSE 'ℹ️'\n",
    "        END as status_icon,\n",
    "        measurement_details\n",
    "    FROM {MAIN_CATALOG}.governance.compliance_metrics\n",
    "    WHERE metric_date >= current_date() - 30\n",
    "    ORDER BY metric_date DESC, metric_name\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.governance.compliance_dashboard WHERE metric_date = current_date()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af56fa84-5aa8-45a3-b32a-87c7bc4f9e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab 5: Governance Automation (60 minutes)\n",
    "\n",
    "### Objectives\n",
    "- Implement policy as code\n",
    "- Create automated compliance checks\n",
    "- Build self-service data access workflows\n",
    "- Set up governance health monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aad6a2e-336c-4960-b6ae-ef91e1d27402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.1: Implement Policy as Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969f6fbb-7d10-4e4d-844e-cad6920fd123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create policy definitions table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.policy_definitions (\n",
    "        policy_id STRING,\n",
    "        policy_name STRING,\n",
    "        policy_type STRING,\n",
    "        policy_rule STRING,\n",
    "        enforcement_level STRING COMMENT 'BLOCK, WARN, or AUDIT',\n",
    "        is_active BOOLEAN,\n",
    "        created_date TIMESTAMP,\n",
    "        created_by STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert policy definitions\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.policy_definitions VALUES\n",
    "    ('P001', 'PII Access Restriction', 'ACCESS_CONTROL', \n",
    "     'Users must be in pii-readers group to access tables tagged with pii=true',\n",
    "     'BLOCK', true, current_timestamp(), 'security_admin'),\n",
    "     \n",
    "    ('P002', 'Cross-Region Data Transfer', 'DATA_MOVEMENT',\n",
    "     'Data tagged with region-specific cannot be accessed outside that region',\n",
    "     'BLOCK', true, current_timestamp(), 'security_admin'),\n",
    "     \n",
    "    ('P003', 'Bulk Data Export Limit', 'EXPORT_CONTROL',\n",
    "     'Exports exceeding 10000 rows require approval',\n",
    "     'WARN', true, current_timestamp(), 'security_admin'),\n",
    "     \n",
    "    ('P004', 'Sensitive Data Masking', 'DATA_PROTECTION',\n",
    "     'SSN and credit card fields must be masked for non-privileged users',\n",
    "     'BLOCK', true, current_timestamp(), 'security_admin'),\n",
    "     \n",
    "    ('P005', 'Audit Trail Requirement', 'COMPLIANCE',\n",
    "     'All access to financial data must be logged',\n",
    "     'AUDIT', true, current_timestamp(), 'compliance_officer')\n",
    "\"\"\")\n",
    "\n",
    "# Create policy evaluation function\n",
    "def evaluate_policy(policy_id, context):\n",
    "    \"\"\"\n",
    "    Evaluate a policy against given context\n",
    "    Returns: (result: bool, message: str)\n",
    "    \"\"\"\n",
    "    policy = spark.sql(f\"\"\"\n",
    "        SELECT * FROM {MAIN_CATALOG}.governance.policy_definitions\n",
    "        WHERE policy_id = '{policy_id}' AND is_active = true\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if not policy:\n",
    "        return True, \"Policy not found or inactive\"\n",
    "    \n",
    "    p = policy[0]\n",
    "    \n",
    "    # Simplified policy evaluation logic\n",
    "    if p.policy_id == 'P001':\n",
    "        # Check PII access\n",
    "        if context.get('table_tags', {}).get('pii') == 'true':\n",
    "            if 'pii-readers' not in context.get('user_groups', []):\n",
    "                return False, \"Access denied: PII data requires pii-readers group membership\"\n",
    "    \n",
    "    elif p.policy_id == 'P003':\n",
    "        # Check export limits\n",
    "        if context.get('export_rows', 0) > 10000:\n",
    "            return False, \"Warning: Large export requires approval\"\n",
    "    \n",
    "    return True, \"Policy check passed\"\n",
    "\n",
    "# Test policy evaluation\n",
    "test_context = {\n",
    "    'table_tags': {'pii': 'true'},\n",
    "    'user_groups': ['analysts'],\n",
    "    'export_rows': 5000\n",
    "}\n",
    "\n",
    "result, message = evaluate_policy('P001', test_context)\n",
    "print(f\"Policy P001 evaluation: {result} - {message}\")\n",
    "\n",
    "log_progress(\"Policy as code framework created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad570b2-222c-472e-a66d-315f3eac5ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.2: Create Automated Compliance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e6d5d0-847c-459c-8b14-64b0ed3fae33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create compliance check results table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.compliance_check_results (\n",
    "        check_id STRING,\n",
    "        check_name STRING,\n",
    "        check_type STRING,\n",
    "        object_name STRING,\n",
    "        check_status STRING,\n",
    "        check_message STRING,\n",
    "        severity STRING,\n",
    "        check_timestamp TIMESTAMP,\n",
    "        remediation_required BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Automated compliance check function\n",
    "def run_compliance_checks():\n",
    "    \"\"\"\n",
    "    Run automated compliance checks across the catalog\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    check_timestamp = datetime.now()\n",
    "    \n",
    "    # Check 1: Tables without classification tags\n",
    "    untagged_tables = spark.sql(f\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM {MAIN_CATALOG}.information_schema.tables\n",
    "        WHERE table_schema NOT IN ('information_schema', 'governance')\n",
    "        LIMIT 10  -- For demo purposes\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Simulate checking for missing tags\n",
    "    for table in untagged_tables[:2]:  # Simulate some tables missing tags\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {MAIN_CATALOG}.governance.compliance_check_results VALUES\n",
    "            ('{uuid.uuid4()}', 'Missing Data Classification', 'TAGGING', \n",
    "             '{table.table_name}', 'FAIL', 'Table missing required classification tags',\n",
    "             'MEDIUM', '{check_timestamp}', true)\n",
    "        \"\"\")\n",
    "    \n",
    "    # Check 2: Unused access grants\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.compliance_check_results\n",
    "        SELECT\n",
    "            '{uuid.uuid4()}' as check_id,\n",
    "            'Unused Access Grant Review' as check_name,\n",
    "            'ACCESS_REVIEW' as check_type,\n",
    "            'Multiple Tables' as object_name,\n",
    "            'WARN' as check_status,\n",
    "            '5 users have table access but no activity in 90 days' as check_message,\n",
    "            'LOW' as severity,\n",
    "            '{check_timestamp}' as check_timestamp,\n",
    "            false as remediation_required\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check 3: Sensitive data without masking\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.compliance_check_results VALUES\n",
    "        ('{uuid.uuid4()}', 'Sensitive Data Protection', 'DATA_PROTECTION',\n",
    "         '{MAIN_CATALOG}.silver.customers.ssn', 'PASS', \n",
    "         'SSN column has appropriate masking function applied',\n",
    "         'HIGH', '{check_timestamp}', false)\n",
    "    \"\"\")\n",
    "    \n",
    "    log_progress(\"Compliance checks completed\")\n",
    "\n",
    "# Run compliance checks\n",
    "run_compliance_checks()\n",
    "\n",
    "# Display results\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT check_name, check_status, object_name, check_message, severity\n",
    "    FROM {MAIN_CATALOG}.governance.compliance_check_results\n",
    "    WHERE check_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
    "    ORDER BY severity DESC, check_status\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29dbcc7a-8f7a-4589-995a-f69c4ee04ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.3: Build Self-Service Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edee6d10-162e-4b60-ad89-a00d309dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create access request workflow tables\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.access_requests (\n",
    "        request_id STRING,\n",
    "        requestor_email STRING,\n",
    "        requested_object STRING,\n",
    "        access_type STRING,\n",
    "        business_justification STRING,\n",
    "        risk_score INT,\n",
    "        request_status STRING,\n",
    "        request_date TIMESTAMP,\n",
    "        approved_by STRING,\n",
    "        approval_date TIMESTAMP,\n",
    "        expiry_date DATE,\n",
    "        auto_approved BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create approval rules table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MAIN_CATALOG}.governance.approval_rules (\n",
    "        rule_id STRING,\n",
    "        object_pattern STRING,\n",
    "        access_type STRING,\n",
    "        max_risk_score INT,\n",
    "        auto_approve BOOLEAN,\n",
    "        approval_group STRING,\n",
    "        max_duration_days INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert approval rules\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {MAIN_CATALOG}.governance.approval_rules VALUES\n",
    "    ('RULE001', '%.sandbox.%', 'SELECT', 100, true, NULL, 90),\n",
    "    ('RULE002', '%.gold.%', 'SELECT', 30, true, NULL, 30),\n",
    "    ('RULE003', '%.silver.%', 'SELECT', 50, false, 'data-stewards', 30),\n",
    "    ('RULE004', '%.bronze.%', 'ALL', 0, false, 'data-engineers', 7)\n",
    "\"\"\")\n",
    "\n",
    "# Self-service access request function\n",
    "def request_data_access(requested_object, access_type, justification):\n",
    "    \"\"\"\n",
    "    Submit a self-service data access request\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    request_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Calculate risk score (simplified)\n",
    "    risk_score = 0\n",
    "    if 'customer' in requested_object.lower() or 'pii' in requested_object.lower():\n",
    "        risk_score += 40\n",
    "    if access_type != 'SELECT':\n",
    "        risk_score += 30\n",
    "    if 'bronze' in requested_object:\n",
    "        risk_score += 20\n",
    "    \n",
    "    # Check approval rules\n",
    "    rules = spark.sql(f\"\"\"\n",
    "        SELECT * FROM {MAIN_CATALOG}.governance.approval_rules\n",
    "        WHERE '{requested_object}' LIKE object_pattern\n",
    "          AND access_type = '{access_type}'\n",
    "          AND max_risk_score >= {risk_score}\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    auto_approved = False\n",
    "    approval_status = 'PENDING'\n",
    "    approved_by = None\n",
    "    \n",
    "    if rules and rules[0].auto_approve:\n",
    "        auto_approved = True\n",
    "        approval_status = 'APPROVED'\n",
    "        approved_by = 'SYSTEM_AUTO_APPROVAL'\n",
    "    \n",
    "    # Insert request\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {MAIN_CATALOG}.governance.access_requests VALUES\n",
    "        ('{request_id}', current_user(), '{requested_object}', '{access_type}',\n",
    "         '{justification}', {risk_score}, '{approval_status}', current_timestamp(),\n",
    "         {f\"'{approved_by}'\" if approved_by else 'NULL'},\n",
    "         {\"current_timestamp()\" if auto_approved else 'NULL'},\n",
    "         current_date() + 30, {auto_approved})\n",
    "    \"\"\")\n",
    "    \n",
    "    return request_id, auto_approved, risk_score\n",
    "\n",
    "# Test self-service access\n",
    "request_id, auto_approved, risk_score = request_data_access(\n",
    "    f\"{MAIN_CATALOG}.gold.sales_summary\",\n",
    "    \"SELECT\",\n",
    "    \"Need access for quarterly business review dashboard\"\n",
    ")\n",
    "\n",
    "print(f\"Access request submitted: {request_id}\")\n",
    "print(f\"Risk score: {risk_score}\")\n",
    "print(f\"Auto-approved: {auto_approved}\")\n",
    "\n",
    "# Display pending requests\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT request_id, requested_object, access_type, risk_score, \n",
    "           request_status, auto_approved\n",
    "    FROM {MAIN_CATALOG}.governance.access_requests\n",
    "    WHERE request_date >= current_timestamp() - INTERVAL 1 HOUR\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6031fa6-48a1-47d5-adb7-f1ed6b8050d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 5.4: Governance Health Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01338a6e-1f70-4487-94a5-2aef9b502730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create governance health metrics view\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {MAIN_CATALOG}.governance.health_dashboard AS\n",
    "    WITH metrics AS (\n",
    "        -- Metric 1: Data Classification Coverage\n",
    "        SELECT \n",
    "            'Data Classification Coverage' as metric_name,\n",
    "            95.0 as metric_value,  -- In production, calculate from actual tags\n",
    "            'PERCENT' as metric_unit,\n",
    "            CASE WHEN 95.0 >= 90 THEN 'GREEN' ELSE 'RED' END as status\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Metric 2: Active Policy Compliance\n",
    "        SELECT \n",
    "            'Active Policy Compliance' as metric_name,\n",
    "            COUNT(CASE WHEN is_active THEN 1 END) * 100.0 / COUNT(*) as metric_value,\n",
    "            'PERCENT' as metric_unit,\n",
    "            'GREEN' as status\n",
    "        FROM {MAIN_CATALOG}.governance.policy_definitions\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Metric 3: Access Request Processing Time\n",
    "        SELECT \n",
    "            'Avg Access Request Time' as metric_name,\n",
    "            2.5 as metric_value,  -- In production, calculate actual average\n",
    "            'HOURS' as metric_unit,\n",
    "            CASE WHEN 2.5 <= 4 THEN 'GREEN' ELSE 'YELLOW' END as status\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Metric 4: Failed Compliance Checks\n",
    "        SELECT \n",
    "            'Failed Compliance Checks (24h)' as metric_name,\n",
    "            COUNT(*) as metric_value,\n",
    "            'COUNT' as metric_unit,\n",
    "            CASE \n",
    "                WHEN COUNT(*) = 0 THEN 'GREEN'\n",
    "                WHEN COUNT(*) <= 5 THEN 'YELLOW'\n",
    "                ELSE 'RED'\n",
    "            END as status\n",
    "        FROM {MAIN_CATALOG}.governance.compliance_check_results\n",
    "        WHERE check_status = 'FAIL'\n",
    "          AND check_timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "    )\n",
    "    SELECT \n",
    "        metric_name,\n",
    "        metric_value,\n",
    "        metric_unit,\n",
    "        status,\n",
    "        CASE \n",
    "            WHEN status = 'GREEN' THEN '🟢'\n",
    "            WHEN status = 'YELLOW' THEN '🟡'\n",
    "            WHEN status = 'RED' THEN '🔴'\n",
    "        END as status_indicator\n",
    "    FROM metrics\n",
    "\"\"\")\n",
    "\n",
    "# Display governance health dashboard\n",
    "display(spark.sql(f\"SELECT * FROM {MAIN_CATALOG}.governance.health_dashboard\"))\n",
    "\n",
    "log_progress(\"Governance automation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ac36a8d-4ce9-48da-a525-48ceb879fb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lab Summary and Cleanup\n",
    "\n",
    "Congratulations! You've successfully implemented a comprehensive data governance solution including:\n",
    "\n",
    "✅ Unity Catalog setup with proper hierarchy\n",
    "✅ Fine-grained security controls (column/row level)\n",
    "✅ Privacy compliance features (GDPR, consent management)\n",
    "✅ Comprehensive audit and lineage tracking\n",
    "✅ Automated governance with self-service capabilities\n",
    "\n",
    "### Key Takeaways\n",
    "1. Unity Catalog provides centralized governance across all data assets\n",
    "2. Fine-grained controls enable secure data sharing\n",
    "3. Automation reduces governance overhead\n",
    "4. Compliance requires continuous monitoring\n",
    "5. Self-service empowers users while maintaining security\n",
    "\n",
    "### Next Steps\n",
    "- Review the governance patterns implemented\n",
    "- Consider how these would apply to your organization\n",
    "- Prepare for Module 6's capstone project\n",
    "- Explore Unity Catalog documentation for advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700d8b7d-a9ce-4bdb-8068-4328afaf9be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Cleanup (DO NOT RUN if you want to keep your work)\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {MAIN_CATALOG} CASCADE\")\n",
    "# print(\"Cleanup complete\")\n",
    "\n",
    "print(\"\\n🎉 Module 5 Labs Complete! 🎉\")\n",
    "print(f\"\\nYour governance catalog '{MAIN_CATALOG}' contains all the implemented features.\")\n",
    "print(\"Ready for Module 6: Capstone Project!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "module5-labs-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
